{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "This is an exercise to be handed in on Peergrade\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def kl_a_calc(q_a,q_param,p_param):\n",
    "    # The function assumes: \n",
    "        # q_a has dimension: [batch_size,sample_size,latent_features]\n",
    "        # q_mu has dimension: [batch_size,latent_features]\n",
    "        # q_mu has dimension: [batch_size,latent_features]\n",
    "    \n",
    "    def log_gaussian(x, mu, log_var):\n",
    "        log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
    "        log_pdf = torch.sum(log_pdf, dim=2) # sum over each over the observations (mu + log_var*epsilon)\n",
    "        log_pdf = torch.sum(log_pdf, dim=1) # sum over the 10 samples from the distribution\n",
    "        return log_pdf\n",
    "\n",
    "    q_mu, q_log_var = q_param\n",
    "    p_mu, p_log_var = p_param\n",
    "    \n",
    "    # put in middle dimension\n",
    "    q_mu = q_mu.unsqueeze(1)\n",
    "    q_log_var = q_log_var.unsqueeze(1)\n",
    "    p_mu = p_mu.unsqueeze_(1)\n",
    "    p_log_var = p_log_var.unsqueeze_(1)\n",
    "    \n",
    "    # densities of each disitribution \n",
    "    qz = log_gaussian(q_a,q_mu,q_log_var)\n",
    "    pz = log_gaussian(q_a,p_mu,p_log_var)\n",
    "    \n",
    "    # kl divergence\n",
    "    kl = qz - pz\n",
    "    \n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  torch.Size([32, 16])\n",
      "After:  torch.Size([32, 1, 16])\n",
      "After:  torch.Size([32, 1, 16])\n",
      "Result:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "q_a = torch.randn(32,10,16)\n",
    "bb_mu = torch.randn(32,16)\n",
    "bb_log_var = torch.randn(32,16)\n",
    "# cc_mu = bb_mu\n",
    "# cc_log_var = bb_log_var\n",
    "cc_mu = torch.randn(32,16)\n",
    "cc_log_var = torch.randn(32,16)\n",
    "\n",
    "result_a = kl_a_calc(q_a,(bb_mu,bb_log_var),(bb_mu,bb_log_var))\n",
    "print('Result: ', result_a.size())\n",
    "# print(result_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([5.])\n",
      "tensor([6.])\n",
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "aa = torch.Tensor([5])\n",
    "print(aa)\n",
    "bb = aa\n",
    "print(bb)\n",
    "aa.add_(1)\n",
    "print(bb)\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-3d10c4c7529f>, line 88)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-3d10c4c7529f>\"\u001b[0;36m, line \u001b[0;32m88\u001b[0m\n\u001b[0;31m    class CNN_VAE(nn.Module):\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU, ELU\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, dropout, dropout2d\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "height = 244 #IMG_SIZE\n",
    "width = 244 #IMG_SIZE\n",
    "channels = 1\n",
    "num_features = 224**2\n",
    "\n",
    "# Regulization\n",
    "L2_reg = 1e-6\n",
    "\n",
    "# 1. Conv Layer\n",
    "conv_out_channels_1 = 4\n",
    "conv_kernel_1 = 5\n",
    "conv_padding_1 = 2\n",
    "conv_stride_1 = 1\n",
    "\n",
    "# 1. MaxPool Layer\n",
    "pool_kernel_1 = conv_kernel_1\n",
    "pool_padding_1 = 2\n",
    "pool_stride_1 = 2\n",
    "\n",
    "# 2. Conv Layer\n",
    "conv_out_channels_2 = 8\n",
    "conv_kernel_2 = 5\n",
    "conv_padding_2 = conv_padding_1\n",
    "conv_stride_2 = 1\n",
    "\n",
    "# 2. MaxPool Layer\n",
    "pool_kernel_2 = 3\n",
    "pool_padding_2 = 0\n",
    "pool_stride_2 = 2\n",
    "\n",
    "\n",
    "# Calculating the dimensions \n",
    "def compute_conv_dim(height, width, kernel_size, padding_size, stride_size):\n",
    "    height_new = int((height - kernel_size + 2 * padding_size) / stride_size + 1)\n",
    "    width_new =  int((width  - kernel_size + 2 * padding_size) / stride_size + 1)\n",
    "    return [height_new, width_new]\n",
    "\n",
    "def compute_final_dimension(height, width, last_num_channels):\n",
    "    # First conv layer\n",
    "    CNN_height, CNN_width = compute_conv_dim(height, width, \n",
    "                                             conv_kernel_1, conv_padding_1, conv_stride_1)\n",
    "    # First maxpool layer\n",
    "    CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, \n",
    "                                             pool_kernel_1, pool_padding_1, pool_stride_1)\n",
    "    # Second conv layer\n",
    "    CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width,\n",
    "                                             conv_kernel_2, conv_padding_2, conv_stride_2)\n",
    "    # Second maxpool layer\n",
    "    CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width,\n",
    "                                             pool_kernel_2, pool_padding_2, pool_stride_2)\n",
    "    final_dim = CNN_height * CNN_width * last_num_channels\n",
    "    return final_dim\n",
    "\n",
    "def gaussian_sample(mu,log_var):    \n",
    "    # Don't propagate gradients through randomness\n",
    "    with torch.no_grad():\n",
    "        batch_size = mu.size(0)\n",
    "        epsilon = torch.randn(batch_size, self.num_samples, self.latent_features)\n",
    "            \n",
    "    if cuda:\n",
    "        epsilon = epsilon.cuda()\n",
    "        \n",
    "    sigma = torch.exp(log_var/2)\n",
    "        \n",
    "    # We will need to unsqueeze to turn\n",
    "    # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n",
    "    z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)\n",
    "    return z\n",
    "\n",
    "def replicate_self(x):\n",
    "        \n",
    "\n",
    "######## Image has to be: (num, channels, height, width)!!!! #########\n",
    "class CNN_VAE(nn.Module):\n",
    "    def __init__(self, latent_features, aux_features, num_samples):\n",
    "        super(CNN_VAE, self).__init__()\n",
    "        \n",
    "        self.latent_features = latent_features\n",
    "        self.num_samples = num_samples\n",
    "        self.aux_features = aux_features\n",
    "        \n",
    "        # Calculate final size of the CNN\n",
    "        self.final_dim = compute_final_dimension(height,width,conv_out_channels_2)\n",
    "\n",
    "        ## CNN encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels_1,\n",
    "                             kernel_size=conv_kernel_1,\n",
    "                             stride=conv_stride_1,\n",
    "                             padding=conv_padding_1),\n",
    "            \n",
    "            MaxPool2d(kernel_size=pool_kernel_1, \n",
    "                             stride=pool_stride_1,\n",
    "                             padding=pool_padding_1),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(conv_out_channels_1),\n",
    "            Dropout2d(p=0.2),\n",
    "            \n",
    "            Conv2d(in_channels=conv_out_channels_1,\n",
    "                             out_channels=conv_out_channels_2,\n",
    "                             kernel_size=conv_kernel_2,\n",
    "                             stride=conv_stride_2,\n",
    "                             padding=conv_padding_2),\n",
    "            \n",
    "            MaxPool2d(kernel_size=pool_kernel_2,\n",
    "                             stride=pool_stride_2,\n",
    "                             padding=pool_padding_2),\n",
    "            ReLU(),\n",
    "            BatchNorm2d(conv_out_channels_2),\n",
    "            Dropout2d(p=0.2)\n",
    "        )\n",
    "        \n",
    "        # Making a \n",
    "        self.CNN_to_aux = nn.Sequential(\n",
    "            Linear(in_features = self.final_dim,out_features=256),\n",
    "            Linear(in_features=256, out_features=self.aux_features*2))\n",
    "        \n",
    "        # Making z \n",
    "        self.CNN_to_latent_1 = Linear(in_features = self.final_dim,out_features=256)\n",
    "        # Notice 256 + aux_features as they are concatenated \n",
    "        self.CNN_to_latent_2 = Linear(in_features=256+aux_features, out_features=self.latent_features*2)\n",
    "        \n",
    "        \n",
    "        # The latent features must be decoded into the original image\n",
    "        self.decoder = nn.Sequential(\n",
    "            Linear(in_features=self.latent_features, out_features=128),\n",
    "            #BatchNorm1d(num_features=128, eps=1e-3),\n",
    "            ELU(),\n",
    "            Linear(in_features=128, out_features=256),\n",
    "            #BatchNorm1d(num_features=256),\n",
    "            ELU(),\n",
    "            Linear(in_features=256, out_features=num_features)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view( batch_size, -1)  # fold out the CNN to vectors \n",
    "        q_a = self.CNN_to_aux(x)     # making a from CNN output \n",
    "        x = self.CNN_to_latent_1(x)  # reduce CNN output to 256 features\n",
    "        z = self.CNN_to_latent_2(torch.cat([x,q_a],dim=2)) # make q(z|x,a)\n",
    "        \n",
    "        # Split encoder outputs into a mean and variance vector\n",
    "        mu, log_var = torch.chunk(x, 2, dim=-1)\n",
    "        \n",
    "        # Make sure that the log variance is positive\n",
    "        log_var = softplus(log_var)\n",
    "        \n",
    "        # :- Reparametrisation trick\n",
    "        # a sample from N(mu, sigma) is mu + sigma * epsilon\n",
    "        # where epsilon ~ N(0, 1)\n",
    "                \n",
    "        # Don't propagate gradients through randomness\n",
    "        with torch.no_grad():\n",
    "            batch_size = mu.size(0)\n",
    "            epsilon = torch.randn(batch_size, self.num_samples, self.latent_features)\n",
    "            \n",
    "        if cuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "        \n",
    "        sigma = torch.exp(log_var/2)\n",
    "        \n",
    "        # We will need to unsqueeze to turn\n",
    "        # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n",
    "        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)  \n",
    "        print('z has size: ',z.size())\n",
    "        \n",
    "        z_f = gaussian_sample(mu,log_var)\n",
    "        print('z_f has size: ',z_f.size())\n",
    "        \n",
    "        # Run through decoder\n",
    "        x = self.decoder(z)\n",
    "        \n",
    "        # The original digits are on the scale [0, 1]\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        # Mean over samples\n",
    "        x_hat = torch.mean(x, dim=1)\n",
    "        \n",
    "        outputs[\"x_hat\"] = x_hat\n",
    "        outputs[\"z\"] = z\n",
    "        outputs[\"mu\"] = mu\n",
    "        outputs[\"log_var\"] = log_var\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "latent_features = 16\n",
    "aux_features = 16\n",
    "# The number of samples used then initialising the VAE, \n",
    "# is number of samples drawn from the distribution\n",
    "num_samples = 10\n",
    "\n",
    "net = CNN_VAE(latent_features, aux_features, num_samples)\n",
    "\n",
    "# Transfer model to GPU if available\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the PyTorch functions for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log gaussian density function:** \n",
    "\n",
    "$logP(x)=-\\frac{1}{2}\\,log(2\\,\\pi)-\\dfrac{log(\\sigma^2)}{2}-\\dfrac{(x-\\mu)^2}{2\\,\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "                   # (q_a_mu,q_a_log_var) \n",
    "def kl_a_calc(q_a,q_par,p_var):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dim' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0ab7d1e78a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'dim' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "dd = torch.Tensor([1,2,3,4,5])\n",
    "print(dd)\n",
    "print(dd.repeat(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch import optim\n",
    "import math\n",
    "\n",
    "def Gaussian_density(sample_img,mu_img,log_var_img):\n",
    "    c = - 0.5 * math.log(2 * math.pi)\n",
    "    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n",
    "    #print(\"Density:\",density)\n",
    "    #print(\"Density.shape:\", density.shape)\n",
    "    return torch.sum(density,dim = 1) # Sum over channels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def Gaussian_density(sample_img,mu_img,log_var_img):\n",
    "    sigma = torch.exp(log_var_img/2)\n",
    "   \n",
    "    density = (math.sqrt(2*math.pi)*sigma)\n",
    "    density = 1/density\n",
    "    tmp = (sample_img - mu_img)**2\n",
    "    tmp2 = 2 * torch.exp(log_var_img)\n",
    "    tmp3 = torch.exp(tmp/tmp2)\n",
    "    density = density * tmp3\n",
    "  \n",
    "    density = 1/(math.sqrt(2*math.pi)*sigma) * torch.exp((sample_img - mu_img)**2/(2 * torch.exp(log_var_img)))\n",
    "    #print(\"Density:\",density)\n",
    "    #print(\"Density.shape:\", density.shape)\n",
    "    return torch.sum(density,dim = 1) # Sum over channels\n",
    "\"\"\"\n",
    "\n",
    "def ELBO_loss(sample_img, mu_img, log_var_img, mu, log_var):\n",
    "    \n",
    "    # Reconstruction error, log[p(x|z)]\n",
    "    # Sum over features\n",
    "        # Old code\n",
    "        #likelihood = -binary_cross_entropy(y, t, reduction=\"none\")\n",
    "        #likelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n",
    "\n",
    "    # New code with guassian density\n",
    "    likelihood = Gaussian_density(sample_img, mu_img, log_var_img)\n",
    "    #likelihood = -binary_cross_entropy(mu_img,sample_img, reduction=\"none\")\n",
    "    #ikelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n",
    "    \n",
    "    # Regularization error: \n",
    "    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n",
    "    # and prior p(z) = N(z | mu, sigma*I).\n",
    "    \n",
    "    # In the case of the KL-divergence between diagonal covariance Gaussian and \n",
    "    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n",
    "    # variance estimator of KL(q||p)\n",
    "    kl = -0.5 * torch.sum(1 + log_var - mu**2 - torch.exp(log_var), dim=1)\n",
    "    \n",
    "    #sigma = torch.exp(log_var/2)\n",
    "    #kl = -0.5 * torch.sum(1 + torch.log(sigma) - mu**2 - torch.exp(log_var), dim=1)\n",
    "    #kl = torch.sum( log_var + mu**2 - torch.exp(log_var/2) - 1, dim=1)\n",
    "\n",
    "    # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "    # mean over batch\n",
    "    print(\"mu.shape\", mu.shape)\n",
    "    print(\"kl.shape:\", kl.shape)\n",
    "    print(\"Likelihood shape:\", likelihood.shape)\n",
    "    tmp = likelihood.view(batch_size, -1)\n",
    "    tmp = torch.sum(tmp, dim=1) # Sum over features (224x224 = 50.176)\n",
    "    print(\"Likelihood mean shape:\", tmp.shape)\n",
    "    print(\"kl.mean shape:\", kl.shape)\n",
    "    \n",
    "    #ELBO = torch.mean(tmp-kl)\n",
    "    ELBO = torch.mean(tmp) - torch.mean(kl)\n",
    "    #ELBO = torch.mean(likelihood) - torch.mean(kl)\n",
    "\n",
    "    \n",
    "    # notice minus sign as we want to maximise ELBO\n",
    "    return -ELBO, kl.sum()\n",
    "\n",
    "# def aux_kld(q_a)\n",
    "\n",
    "\n",
    "# define our optimizer\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay= 1e-4)\n",
    "loss_function = ELBO_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch import optim\n",
    "import math\n",
    "\n",
    "    # sums the likelihood over all samples images\n",
    "    # so that the output is [height,width]\n",
    "def Gaussian_density(sample_img,mu_img,log_var_img):\n",
    "    c = - 0.5 * math.log(2 * math.pi)\n",
    "    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n",
    "    return torch.sum(density,dim = 0) # \n",
    "\n",
    "def ELBO_loss(sample_img, mu_img, log_var_img, mu, log_var):\n",
    "    \n",
    "    # Reconstruction error, log[p(x|z)]\n",
    "    # Sum over features\n",
    "        # Old code\n",
    "        #likelihood = -binary_cross_entropy(y, t, reduction=\"none\")\n",
    "        #likelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n",
    "\n",
    "    # New code with guassian density\n",
    "    likelihood = Gaussian_density(sample_img, mu_img, log_var_img)\n",
    "    \n",
    "    # Regularization error: \n",
    "    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n",
    "    # and prior p(z) = N(z | mu, sigma*I).\n",
    "    \n",
    "    # In the case of the KL-divergence between diagonal covariance Gaussian and \n",
    "    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n",
    "    # variance estimator of KL(q||p)\n",
    "    kl = -0.5 * torch.sum(1 + log_var - mu**2 - torch.exp(log_var), dim=1)\n",
    "\n",
    "    # Combining the two terms in the evidence lower bound objective (ELBO) \n",
    "    # mean over batch\n",
    "    ELBO = torch.mean(likelihood) - torch.mean(kl)\n",
    "    \n",
    "    # notice minus sign as we want to maximise ELBO\n",
    "    return -ELBO, kl.sum()\n",
    "\n",
    "\n",
    "# define our optimizer\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = ELBO_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
