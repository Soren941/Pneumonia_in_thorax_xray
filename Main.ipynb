{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7339e0251cd57e30522b3be129925a03fac8566b"
      },
      "cell_type": "code",
      "source": "# Questions\n# 1. How do we avoid NaN in the output from Encoder?\n# 2. Is loss function correct? \n#    - How do we avoid normal distributed reconstructionS?\n# 3. Do KL divergence only work under the assumption that covariance is diagonal?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fd1675be6a320e69d8e7bb772717a3faf4f7820"
      },
      "cell_type": "code",
      "source": "! pip install pandas\n! pip install pydicom\n! pip install seaborn\n! pip install memory_profiler\n%load_ext memory_profiler\n\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom skimage.transform import resize\nfrom skimage.exposure import equalize_hist\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, clear_output\nimport numpy as np\n%matplotlib nbagg\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(sns.dark_palette(\"purple\"))\n\nimport torch\ncuda = torch.cuda.is_available()\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom functools import reduce\n\nimport torch.nn as nn\nfrom torch.nn.functional import softplus\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom itertools import cycle\nimport operator\nfrom torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU, ELU, ConvTranspose2d, MaxUnpool2d, Softmax, Sigmoid\nfrom torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, dropout, dropout2d\nimport time\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (0.23.4)\nRequirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.15.4)\nRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.4)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\nRequirement already satisfied: pydicom in /opt/conda/lib/python3.6/site-packages (1.2.1)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.6/site-packages (0.9.0)\nRequirement already satisfied: numpy>=1.9.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.15.4)\nRequirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.1.0)\nRequirement already satisfied: matplotlib>=1.4.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: pandas>=0.15.2 in /opt/conda/lib/python3.6/site-packages (from seaborn) (0.23.4)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.6.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2018.4)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.2.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.1.0)\nRequirement already satisfied: memory_profiler in /opt/conda/lib/python3.6/site-packages (0.54.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from memory_profiler) (5.4.8)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "3f518499954d2ad60289c06138d796a17a6f5aee"
      },
      "cell_type": "code",
      "source": "# Show files in data\n#!ls data\n\nKAGGLE = True\n\n# display label format\nif KAGGLE:\n    df = pd.read_csv('../input/stage_2_train_labels.csv')\nelse:\n    df = pd.read_csv('data/stage_1_train_labels.csv')\n\n\n\n# Parameters\nbatch_size = 32\nNo_train_samples = 1024\nNo_test_samples = 512\nclasses = [0, 1] \nlabels_per_class = 64 # Specify how many labelled examples we want per digit class\nNo_train_labelled_samples = labels_per_class*len(classes)\n\nNo = 4\npatientId = df['patientId'][No]\nif KAGGLE:\n    dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId\nelse:\n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId\ndcm_data = pydicom.read_file(dcm_file)\n#print(df.iloc[No])\n\n\nIMG_SIZE = 224\nimg_dimension = [IMG_SIZE,IMG_SIZE] # New size of xray images. \nunq, idx = np.unique(df['patientId'], return_index = True) # Get only unique entrances from the provided data (some patients occur multiple times)\n\n# Reshape images and match to corresponding label in new dataframe\nTarget = []\nImage = []\nDo_img_eq = True\n\nprint_every = 100\n\nelapsed_time = 0\ntic = time.clock()\n\nprint(\"Loading training images: 0 /\", No_train_samples)\nfor i in range(0,No_train_samples):\n    Target.append(df.Target[idx[i]]) # Get label  \n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i,\"/\",No_train_samples,  \"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();   \nprint(\"Train data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n        \nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\nfor i in range(0,No_test_samples):\n    Target.append(df.Target[idx[No_train_samples+i]]) # Get label  \n    patientId = df['patientId'][idx[No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_test_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_test_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i, \"/\",No_test_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n\nprint(\"Test data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntest_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n\nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\ncount_unlabelled = 0\ncount_labelled = 0\ni = 0\nwhile count_labelled<labels_per_class or count_unlabelled<labels_per_class:\n#for i in range(0,No_train_labelled_samples):\n    Target.append(df.Target[idx[No_test_samples+No_train_samples+i]]) # Get label  \n\n    if Target[i]==1:\n        count_labelled = count_labelled + 1\n    else:\n        count_unlabelled = count_unlabelled + 1\n        \n    patientId = df['patientId'][idx[No_test_samples+No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i, \"/\",No_train_labelled_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n    i = i + 1\n\nprint(\"Labelled test data loaded:\", i+1)\n\n\ndef uniform_stratified_sampler(labels, n=None):\n    \"\"\"\n    Stratified sampler that distributes labels uniformly by\n    sampling at most n data points per class\n    \"\"\"\n    from functools import reduce\n    # Only choose digits in n_labels\n    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n\n    # Ensure uniform distribution of labels\n    np.random.shuffle(indices)\n    indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in classes])\n\n    indices = torch.from_numpy(indices)\n    sampler = SubsetRandomSampler(indices)\n    return sampler\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader_labelled = DataLoader(loader, batch_size=batch_size,\n                     sampler=uniform_stratified_sampler(Target, labels_per_class))\n\nalpha = No_train_labelled_samples / No_train_samples\ndel df, idx, unq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading training images: 0 / 1024\nData loaded: 100 / 1024     Remaining time:  0 : 54\nData loaded: 200 / 1024     Remaining time:  0 : 48\nData loaded: 300 / 1024     Remaining time:  0 : 42\nData loaded: 400 / 1024     Remaining time:  0 : 36\nData loaded: 500 / 1024     Remaining time:  0 : 30\nData loaded: 600 / 1024     Remaining time:  0 : 24\nData loaded: 700 / 1024     Remaining time:  0 : 18\nData loaded: 800 / 1024     Remaining time:  0 : 12\nData loaded: 900 / 1024     Remaining time:  0 : 6\nData loaded: 1000 / 1024     Remaining time:  0 : 0\nTrain data loaded: 1024\n \nData loaded: 100 / 512     Remaining time:  0 : 24\nData loaded: 200 / 512     Remaining time:  0 : 18\nData loaded: 300 / 512     Remaining time:  0 : 12\nData loaded: 400 / 512     Remaining time:  0 : 6\nData loaded: 500 / 512     Remaining time:  0 : 0\nTest data loaded: 512\n \nData loaded: 100 / 128     Remaining time:  0 : 0\nData loaded: 200 / 128     Remaining time:  -1 : 54\nLabelled test data loaded: 214\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "586fcd78be4c8ec2b945248d87601cf271a9ee42"
      },
      "cell_type": "code",
      "source": "# Show an example image \nim = Image[5]\nim = im.squeeze(0)\npylab.imshow(im, cmap=pylab.cm.gist_gray)\npylab.axis('off')\n\ntorch.cuda.max_memory_cached(device=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5bb84df76e774d89d86fe10741f9cf7e0748d531"
      },
      "cell_type": "code",
      "source": "# Histogram Equalization\n\"\"\"\nfrom IPython.display import Image\nfrom skimage import exposure\nimport pylab\n\nImage = []\nrow_int = []\n\nfor i in range(0,16):\n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect', anti_aliasing=True)) # resize image\n    tmp = equalize_hist(Image[-1])\n    row_int.append(np.mean(tmp[:,40:180],axis=1))\n\n# Plotting of images\nf, axarr = plt.subplots(1, 1, figsize=(50, 25))\n\ncolumns = 10\ncanvas = np.zeros((columns*IMG_SIZE, IMG_SIZE*3))\nfor j in range(columns):\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 0:IMG_SIZE] = Image[j]\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , IMG_SIZE:2*IMG_SIZE] = equalize_hist(Image[j])\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 2*IMG_SIZE:3*IMG_SIZE] = np.matrix.transpose(np.tile(row_int[j],(IMG_SIZE,1)))\n\nplt.imshow(canvas, cmap='gray')\nax = axarr\nax.set_title('Original Images')\nax.axis('off')\nax.grid(False)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05778375fb32b8b3a0a2e5b72822f6db8061e7fb",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Define size variables\nprint_shapes = False\nIMG_SIZE = 224\nheight = IMG_SIZE\nwidth = IMG_SIZE\nchannels = 1\nnum_features = 224**2\n\n# Regulization\nL2_reg = 1e-6\ndo_p = 0.2\n\n# Conv Layers\nconv_out_channels = [8, 16, 32]\nconv_kernel = [5, 5, 3]\nconv_padding = [2, 2, 1]\nconv_stride = [1, 1, 1]\n\n# MaxPool Layers\npool_kernel = 3\npool_padding = 0\npool_stride = 3\n\n# Fully connected layers\nlin_layer = [200, 200]\n\n# auxillary parameters\naux_layer = [200, 200]\naux_variables = 32\naux_in = 2 # layer no. where a is included in encoder\naux_decoder_layers = [100, 100]\n\n# classifier parameters\nclassifier_layer = [200, 100]\nNo_classes = len(classes)\n\n# No. of layes\nNUM_CONV = len(conv_out_channels)\nNUM_LIN = len(lin_layer)\nNUM_AUX = len(aux_layer)\nNUM_CLASS = len(classifier_layer)\nNUM_AUX_DECODER = len(aux_decoder_layers)\n\n# Calculating the dimensions \ndef compute_conv_dim(height, width, kernel_size, padding_size, stride_size):\n    height_new = int((height - kernel_size + 2 * padding_size) / stride_size + 1)\n    width_new =  int((width  - kernel_size + 2 * padding_size) / stride_size + 1)\n    return [height_new, width_new]\n\ndef compute_final_dimension(height, width, last_num_channels, num_layers):\n    # First conv layer\n    CNN_height = height\n    CNN_width = width\n    for i in range(num_layers):\n        # conv layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, conv_kernel[i], conv_padding[i], conv_stride[i])\n        # maxpool layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, pool_kernel, pool_padding, pool_stride)\n    final_dim = CNN_height * CNN_width * last_num_channels\n    print(final_dim,CNN_height,CNN_width)\n    return [final_dim, CNN_height, CNN_width]\n\ndef normalize(x):\n    tmp = x-torch.min( torch.min(x,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0]\n    if torch.sum(torch.isnan(tmp))>0:\n        print(\"nan of tmp\",torch.sum(torch.isnan(tmp)))\n    return tmp/(torch.max( torch.max(tmp,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0] + 1e-8)  \n\ndef gaussian_sample(mu,log_var, num_samples, latent_features):    \n    # Don't propagate gradients through randomness\n    with torch.no_grad():\n        batch_size = mu.size(0)\n        epsilon = torch.randn(batch_size, num_samples, latent_features)\n            \n    if cuda:\n        epsilon = epsilon.cuda()\n        \n    sigma = torch.exp(log_var/2)\n        \n    # We will need to unsqueeze to turn\n    # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n    if len(mu.shape) == 2:\n        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)\n    else:\n        z = mu + epsilon * sigma\n    return z\n\n######## Image has to be: (num, channels, height, width)!!!! #########\nclass CNN_VAE(nn.Module):\n    \n    def __init__(self, latent_features, num_samples):\n        super(CNN_VAE, self).__init__()\n        \n        self.latent_features = latent_features\n        self.num_samples = num_samples\n        \n        # Calculate final size of the CNN\n        self.final_dim = compute_final_dimension(height,width,conv_out_channels[-1],NUM_CONV)\n        \n        ## Convolutional layers of the encoder\n        input_channels = channels\n        self.encoder = nn.ModuleList()\n        for i in range(NUM_CONV):\n            self.encoder.append(Conv2d( in_channels=input_channels,\n                                            out_channels=conv_out_channels[i],\n                                            kernel_size=conv_kernel[i],\n                                            stride=conv_stride[i],\n                                            padding=conv_padding[i]))\n            self.encoder.append(BatchNorm2d(conv_out_channels[i]))\n            self.encoder.append(ReLU())\n            self.encoder.append(Dropout2d(p=do_p))\n            self.encoder.append(MaxPool2d(  kernel_size=pool_kernel, \n                                        stride=pool_stride,\n                                        padding=pool_padding,\n                                        return_indices = False))\n            input_channels = conv_out_channels[i]\n        self.add_module(\"encoder\",self.encoder)\n        \n        # Fully connected layers from convolutional layers to latent space\n        self.CNN_to_latent = nn.ModuleList()\n        for i in range(NUM_LIN):\n            if i == 0:\n                in_weights = self.final_dim[0]\n            else:\n                in_weights = lin_layer[i-1]\n            self.CNN_to_latent.append(Linear(in_features=in_weights, out_features=lin_layer[i]))\n            self.CNN_to_latent.append(BatchNorm1d(lin_layer[i]))\n            self.CNN_to_latent.append(ReLU())\n            self.CNN_to_latent.append(Dropout(p=do_p))\n        self.CNN_to_latent.append(Linear(in_features=lin_layer[-1]+aux_variables+No_classes, out_features=latent_features*2))\n        self.CNN_to_latent.append(BatchNorm1d(latent_features*2))\n        self.CNN_to_latent.append(ReLU())\n        self.CNN_to_latent.append(Dropout(p=do_p))\n        self.add_module(\"CNN_to_latent\",self.CNN_to_latent)\n        \n        # Fully connected layers from convolutional layers to aux. variables\n        if aux_variables > 0:\n            self.CNN_to_aux = nn.ModuleList()\n            for i in range(NUM_AUX):\n                if i == 0:\n                    in_weights = self.final_dim[0]\n                else:\n                    in_weights = aux_layer[i-1]\n                self.CNN_to_aux.append(Linear(in_features=in_weights, out_features=aux_layer[i]))\n                self.CNN_to_aux.append(BatchNorm1d(aux_layer[i]))\n                self.CNN_to_aux.append(ReLU())\n                self.CNN_to_aux.append(Dropout(p=do_p))\n\n            self.CNN_to_aux.append(Linear(in_features=aux_layer[-1], out_features=aux_variables*2))\n            self.CNN_to_aux.append(BatchNorm1d(aux_variables*2))\n            self.CNN_to_aux.append(ReLU())\n            self.CNN_to_aux.append(Dropout(p=do_p))\n            self.add_module(\"CNN_to_aux\", self.CNN_to_aux)\n        \n        # auxillary decoder\n        if aux_variables > 0:\n            self.aux_decoder = nn.ModuleList()\n            for i in range(NUM_AUX_DECODER):\n                if i == 0:\n                    in_weights = self.latent_features + lin_layer[-1] + No_classes\n                else:\n                    in_weights = aux_decoder_layers[i-1]\n                self.aux_decoder.append(Linear(in_features=in_weights, out_features=aux_decoder_layers[i]))\n                self.aux_decoder.append(BatchNorm1d(aux_decoder_layers[i]))\n                self.aux_decoder.append(ReLU())\n                self.aux_decoder.append(Dropout(p=do_p))\n\n            self.aux_decoder.append(Linear(in_features=aux_decoder_layers[-1], out_features=aux_variables*2))\n            self.aux_decoder.append(BatchNorm1d(aux_variables*2))\n            self.aux_decoder.append(ReLU())\n            self.aux_decoder.append(Dropout(p=do_p))\n            self.add_module(\"aux_decoder\", self.aux_decoder)    \n        \n        # Initialize fully connected layers from latent space to convolutional layers\n        self.latent_to_CNN = nn.ModuleList()\n        self.latent_to_CNN.append(Linear(in_features=latent_features+No_classes, out_features=lin_layer[-1]))\n        self.latent_to_CNN.append(BatchNorm1d(lin_layer[-1]))\n        self.latent_to_CNN.append(ReLU())\n        self.latent_to_CNN.append(Dropout(p=do_p))\n        for i in reversed(range(NUM_LIN)):\n            if i == 0:\n                out_weights = self.final_dim[0]\n            else:\n                out_weights = lin_layer[i-1]\n            self.latent_to_CNN.append(Linear(in_features=lin_layer[i], out_features=out_weights))\n            self.latent_to_CNN.append(BatchNorm1d(out_weights))\n            self.latent_to_CNN.append(ReLU())\n            self.latent_to_CNN.append(Dropout(p=do_p))\n        self.add_module(\"latent_to_CNN\",self.latent_to_CNN)\n        \n        # Convolutional layers of the decoder\n        self.decoder = nn.ModuleList()\n        for i in reversed(range(NUM_CONV)):\n            if i == 0:\n                output_channels = channels*2\n            else:\n                output_channels = conv_out_channels[i-1] \n            self.decoder.append(ConvTranspose2d(in_channels=conv_out_channels[i],\n                                                out_channels=output_channels,\n                                                kernel_size=conv_kernel[i],\n                                                stride=conv_stride[i],\n                                                padding=conv_padding[i]))\n            self.decoder.append(BatchNorm2d(output_channels))\n            self.decoder.append(ReLU())\n            self.decoder.append(Dropout2d(p=do_p))                        \n        self.add_module(\"encoder\",self.encoder)\n\n        # Fully connected layers from convolutional layers to classification\n        self.classifier = nn.ModuleList()\n        for i in range(NUM_CLASS):\n            if i == 0:\n                if aux_variables > 0:\n                    in_weights = lin_layer[-1]+aux_variables\n                else:\n                    in_weights = lin_layer[-1]\n            else:\n                in_weights = classifier_layer[i-1]\n            self.classifier.append(Linear(in_features=in_weights, out_features=classifier_layer[i]))\n            #self.classifier.append(BatchNorm1d(classifier_layer[i]))\n            self.classifier.append(ReLU())\n            self.classifier.append(Dropout(p=do_p))  \n\n        self.classifier.append(Linear(in_features=classifier_layer[-1], out_features = No_classes))\n        self.classifier.append(Softmax())\n        self.add_module(\"classifier\", self.classifier)\n        \n        \n### Forward ####\n    def forward(self, x, y=None):\n        outputs = {}\n        self.indices = []\n        self.layer_size = []\n    \n    \n    # convolutional layers of encoder\n        for i in range(len(self.encoder)):\n            if str(type(self.encoder[i])) == \"<class 'torch.nn.modules.pooling.MaxPool2d'>\":\n                self.layer_size.append(x.shape[-1])\n            x = self.encoder[i](x)  \n\n        x = x.view(batch_size, -1)\n        \n    # Auxillary linear layers\n        if aux_variables > 0:\n            a = x\n            for i in range(len(self.CNN_to_aux)):\n                a = self.CNN_to_aux[i](a)\n            aux_mu, aux_log_var = torch.chunk(a, 2, dim=-1) # divide to mu and sigma\n            a = gaussian_sample(aux_mu,aux_log_var,num_samples,aux_variables) # sample auxillary variables\n            outputs[\"q_a\"] = a\n            \n    # Linear layers to map to latent space\n        for i in range(len(self.CNN_to_latent)-4): \n            x = self.CNN_to_latent[i](x)\n        x_aux = x    \n        if aux_variables > 0:\n            x = torch.cat([x.unsqueeze(1).repeat(1,num_samples,1),a],dim=2)\n        else:\n            x = torch.unsqueeze(x,dim = 1)\n        c = x # the variable that should be used in the classifier.\n\n        if y is None:\n            x_UL = []\n            for j in range(No_classes):\n                tmp = Variable(torch.zeros(2))\n                tmp[j] = 1\n                if cuda:\n                    tmp = tmp.cuda()\n                x = c.view(-1,c.shape[-1])\n                x = torch.cat([x,tmp.unsqueeze(0).repeat(x.shape[0],1)],dim=-1)\n                for i in range(len(self.CNN_to_latent)-4, len(self.CNN_to_latent)):\n                    x = self.CNN_to_latent[i](x)\n                x_UL.append(x)\n            x = sum(x_UL)\n            del x_UL, tmp\n        else:\n            x = torch.cat([x,y.unsqueeze(1).repeat(1,x.shape[1],1)],dim=-1)\n            x = x.view(-1,x.shape[-1])\n            for i in range(len(self.CNN_to_latent)-4, len(self.CNN_to_latent)):\n                x = self.CNN_to_latent[i](x)\n\n        # include y and sum after last layer \n        x = x.view(batch_size,-1,latent_features*2) # make correct dimensions\n        \n        # Split encoder outputs into a mean and variance vector\n        mu, log_var = torch.chunk(x, 2, dim=-1)\n        \n        # Make sure that the log variance is positive\n        log_var = softplus(log_var)\n        \n        # Sample\n        z = gaussian_sample(mu,log_var,num_samples,latent_features)\n        \n        # aux. decoder\n        if aux_variables > 0:\n            x_aux = torch.cat([z, x_aux.unsqueeze(1).repeat(1,z.shape[1],1)],dim = -1)\n            if y is None:\n                a_UL = []\n                for j in range(No_classes):\n                    tmp = Variable(torch.zeros(2))\n                    tmp[j] = 1\n                    if cuda:\n                        tmp = tmp.cuda()\n                    a = x_aux.view(-1,x_aux.shape[-1])\n                    a = torch.cat([a,tmp.unsqueeze(0).repeat(a.shape[0],1)],dim=-1)\n                    for i in range(len(self.aux_decoder)):\n                        a = self.aux_decoder[i](a)\n                    a_UL.append(a)\n                a = sum(a_UL)\n                del a_UL, tmp\n            else:\n                a = torch.cat([x_aux,y.unsqueeze(1).repeat(1,x.shape[1],1)],dim=-1)\n                a = a.view(-1,a.shape[-1])\n                for i in range(len(self.aux_decoder)):\n                    a = self.aux_decoder[i](a)\n                    \n            a = a.view(batch_size,num_samples,2,-1)\n            a_mean, a_log_var = torch.chunk(a, 2, dim=2) # the mean and log_var reconstructions from the decoder\n            a_log_var = softplus(a_log_var)\n        \n        # linear layers of decoder     \n        if y is None:\n            x_UL = []\n            for j in range(No_classes):\n                tmp = Variable(torch.zeros(2))\n                tmp[j] = 1\n                if cuda:\n                    tmp = tmp.cuda()\n                x = z.view(-1,latent_features)\n                x = torch.cat([x,tmp.unsqueeze(0).repeat(x.shape[0],1)],dim=1)\n           # linear layers to CNN     \n                for i in range(len(self.latent_to_CNN)):\n                    x = self.latent_to_CNN[i](x)\n                x = x.view(-1, conv_out_channels[NUM_CONV-1], self.final_dim[1], self.final_dim[2])\n            # Run through decoder\n                curr_layer = NUM_CONV-1\n                for i in range(len(self.decoder)):\n                    if str(type(self.decoder[i])) == \"<class 'torch.nn.modules.conv.ConvTranspose2d'>\":\n                        upsample = nn.Upsample(size = [self.layer_size[curr_layer],self.layer_size[curr_layer]],\n                                              mode = 'bilinear', \n                                              align_corners = False)\n                        x = upsample(x)\n                        curr_layer -=1\n                    x = self.decoder[i](x)\n                x_UL.append(x.view(batch_size,-1,channels*2,height,width))\n            x = sum(x_UL)\n            del x_UL, tmp\n        else:\n            x = torch.cat([z,y.unsqueeze(1).repeat(1,z.shape[1],1)],dim=2)\n            x = x.view(-1,x.shape[-1])\n        # linear layers to CNN     \n            for i in range(len(self.latent_to_CNN)):\n                x = self.latent_to_CNN[i](x)\n            x = x.view(-1, conv_out_channels[NUM_CONV-1], self.final_dim[1], self.final_dim[2])\n        # Run through decoder\n            curr_layer = NUM_CONV-1\n            for i in range(len(self.decoder)):\n                if str(type(self.decoder[i])) == \"<class 'torch.nn.modules.conv.ConvTranspose2d'>\":\n                    upsample = nn.Upsample(size = [self.layer_size[curr_layer],self.layer_size[curr_layer]],\n                                          mode = 'bilinear', \n                                          align_corners = False)\n                    x = upsample(x)\n                    curr_layer -=1\n                x = self.decoder[i](x)\n            x = x.view(batch_size,-1,channels*2,height,width)    \n        \n        x_mean, x_log_var = torch.chunk(x, 2, dim=2) # the mean and log_var reconstructions from the decoder\n\n        # The original digits are on the scale [0, 1] \n        x_hat = normalize(x_mean)# to scale for showing an image\n        x_log_var = softplus(x_log_var)\n        \n        # Mean over samples\n        x_hat = torch.mean(x_hat, dim=1)\n        x_log_var= torch.mean(x_log_var, dim=1)\n        \n        # Resize x_hat from [batch_size, no_features] to [batch_size, channels, height, width]\n        x_hat = x_hat.view( batch_size, 1, height, width)\n        x_log_var = x_log_var.view( batch_size, 1, height, width)\n        \n        # Run Classifier\n        for i in range(len(self.classifier)):\n            c = self.classifier[i](c)\n        c = softmax(c,dim = -1)\n        \n        # Assign variables\n        outputs[\"x_hat\"] = x_hat # This is used for visulizations only \n        outputs[\"z\"] = z\n        outputs[\"mu\"] = mu\n        outputs[\"log_var\"] = log_var\n        \n        # image recontructions (notice they are outputted as matrices)\n        outputs[\"x_mean\"] = x_hat #torch.reshape(x_mean,(-1,height,width)) # mean reconstructions (for loss!!!)\n        outputs[\"x_log_var\"] = x_log_var #torch.reshape(x_log_var,(-1,height,width)) # log var reconstructions (for loss!!!)\n        \n        \n        # auxillary outputs\n        if aux_variables > 0:            \n            outputs[\"q_a_mu\"] = aux_mu\n            outputs[\"q_a_log_var\"] = aux_log_var\n            outputs[\"p_a_mu\"] = a_mean\n            outputs[\"p_a_log_var\"] = a_log_var\n        \n        # classifier outputs \n        outputs[\"y_hat\"] = c\n            \n        return outputs\n\n# The number of samples used then initialising the VAE, \n# is number of samples drawn from the distribution\nnum_samples = 10\nlatent_features = 64\n\nnet = CNN_VAE(latent_features, num_samples)\nprint(net)\n# Transfer model to GPU ifavailable\nif cuda:\n    net = net.cuda()\n\n## test\nx = torch.randn(batch_size,1,224,224)\nx = Variable(x)\nif cuda:\n    x = x.cuda()\n    y = None\ny = net(x)\nprint(y['x_hat'].shape)\n#for parameter in net.parameters():\n#    print(parameter.shape)\n#epsilon = torch.randn(batch_size, latent_features).cuda\n#samples = torch.sigmoid(net.decoder(net.latent_to_CNN(epsilon))).detach()",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2048 8 8\nCNN_VAE(\n  (encoder): ModuleList(\n    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout2d(p=0.2)\n    (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): ReLU()\n    (8): Dropout2d(p=0.2)\n    (9): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU()\n    (13): Dropout2d(p=0.2)\n    (14): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n  )\n  (CNN_to_latent): ModuleList(\n    (0): Linear(in_features=2048, out_features=200, bias=True)\n    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2)\n    (4): Linear(in_features=200, out_features=200, bias=True)\n    (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2)\n    (8): Linear(in_features=234, out_features=128, bias=True)\n    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2)\n  )\n  (CNN_to_aux): ModuleList(\n    (0): Linear(in_features=2048, out_features=200, bias=True)\n    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2)\n    (4): Linear(in_features=200, out_features=200, bias=True)\n    (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2)\n    (8): Linear(in_features=200, out_features=64, bias=True)\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2)\n  )\n  (aux_decoder): ModuleList(\n    (0): Linear(in_features=266, out_features=100, bias=True)\n    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2)\n    (4): Linear(in_features=100, out_features=100, bias=True)\n    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2)\n    (8): Linear(in_features=100, out_features=64, bias=True)\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2)\n  )\n  (latent_to_CNN): ModuleList(\n    (0): Linear(in_features=66, out_features=200, bias=True)\n    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.2)\n    (4): Linear(in_features=200, out_features=200, bias=True)\n    (5): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2)\n    (8): Linear(in_features=200, out_features=2048, bias=True)\n    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout(p=0.2)\n  )\n  (decoder): ModuleList(\n    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout2d(p=0.2)\n    (4): ConvTranspose2d(16, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): Dropout2d(p=0.2)\n    (8): ConvTranspose2d(8, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (9): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n    (11): Dropout2d(p=0.2)\n  )\n  (classifier): ModuleList(\n    (0): Linear(in_features=232, out_features=200, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2)\n    (3): Linear(in_features=200, out_features=100, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2)\n    (6): Linear(in_features=100, out_features=2, bias=True)\n    (7): Softmax()\n  )\n)\ntorch.Size([32, 1, 224, 224])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd92a1ecff915e0aa69a9a2613ef672228917283"
      },
      "cell_type": "code",
      "source": "from torch.nn.functional import binary_cross_entropy\nfrom torch import optim\nimport math\n\ndef Gaussian_density(sample_img,mu_img,log_var_img):\n    c = - 0.5 * math.log(2 * math.pi)\n    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n    #print(\"Density:\",density)\n    #print(\"Density.shape:\", density.shape)\n    return torch.sum(density,dim = 1) # Sum over channels\n\ndef kl_a_calc(q_a,q_mu, q_log_var,p_mu, p_log_var):\n    # The function assumes: \n        # q_a has dimension: [batch_size,sample_size,latent_features]\n        # q_mu has dimension: [batch_size,latent_features]\n        # p_mu has dimension: [batch_size,latent_features]\n    p_mu = torch.squeeze(torch.sum(p_mu, dim = 1), dim = 1)\n    p_log_var = torch.squeeze(torch.sum(p_log_var, dim = 1), dim = 1)\n    def log_gaussian(x, mu, log_var):\n        log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n        log_pdf = torch.sum(log_pdf, dim=1) # sum over each over the observations (mu + log_var*epsilon)\n        #log_pdf = torch.sum(log_pdf, dim=1) # sum over the 10 samples from the distribution\n        return log_pdf\n\n    #q_mu, q_log_var = q_param\n    #p_mu, p_log_var = p_param\n    \n    # put in middle dimension\n    q_mu = q_mu.unsqueeze(1)\n    q_log_var = q_log_var.unsqueeze(1)\n    p_mu = p_mu.unsqueeze_(1)\n    p_log_var = p_log_var.unsqueeze_(1)\n    \n    # densities of each disitribution \n    qz = log_gaussian(q_a,q_mu,q_log_var)\n    pz = log_gaussian(q_a,p_mu,p_log_var)\n    \n    # kl divergence\n    kl = qz - pz\n    \n    return kl\n\ndef ELBO_loss(sample_img, outputs):\n    w1 = 0.5\n    w2 = 0.0005\n#def ELBO_loss(sample_img, mu_img, log_var_img, mu, log_var):\n    \n    # Reconstruction error, log[p(x|z)]\n    # Sum over features\n        # Old code\n        #likelihood = -binary_cross_entropy(y, t, reduction=\"none\")\n        #likelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n\n    # New code with guassian density\n    likelihood = Gaussian_density(sample_img, outputs['x_mean'], outputs['x_log_var'])\n    #likelihood = -binary_cross_entropy(mu_img,sample_img, reduction=\"none\")\n    #ikelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n    \n    # Regularization error: \n    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n    # and prior p(z) = N(z | mu, sigma*I).\n    \n    # In the case of the KL-divergence between diagonal covariance Gaussian and \n    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n    # variance estimator of KL(q||p)\n    #kl = -0.5 * torch.sum(1 + log_var - mu**2 - torch.exp(log_var), dim=1)\n    kl_x = -0.5 * torch.sum(1 + outputs['log_var'] - outputs['mu']**2 - torch.exp(outputs['log_var']), dim=1)\n    if aux_variables > 0:\n        kl_a = kl_a_calc(outputs[\"q_a\"],outputs[\"q_a_mu\"],outputs[\"q_a_log_var\"],outputs[\"p_a_mu\"],outputs[\"p_a_log_var\"])\n    else:\n        kl_a = 0\n    # Combining the two terms in the evidence lower bound objective (ELBO) \n    # mean over batch\n    #print(\"mu.shape\", mu.shape)\n    #print(\"kl.shape:\", kl.shape)\n    #print(\"Likelihood shape:\", likelihood.shape)\n    tmp = likelihood.view(batch_size, -1)\n    tmp = torch.sum(tmp, dim=1) # Sum over features (224x224 = 50.176)\n    #print(\"Likelihood sum shape:\", tmp.shape)\n    #print(\"kl.mean shape:\", kl.shape)\n    kl = w1 * torch.mean(kl_x) + (1 - w1) * torch.mean(kl_a)\n    ELBO = w2 * torch.mean(tmp) - (1 - w2) * kl\n\n    # Notice minus sign as we want to maximise ELBO\n    return -ELBO, (1 - w2) * kl\n\n\n# Define optimizer: The Adam optimizer works really well with VAEs.\noptimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay= 1e-4)\nloss_function = ELBO_loss",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "7e691c43f146ec4fb7e8a5157b35d650ecb8fbef"
      },
      "cell_type": "code",
      "source": "from torch.autograd import Variable\n\nx, y = next(iter(train_loader_labelled))\nu, _ = next(iter(train_loader))\n\ny_hot =  torch.zeros([batch_size,2], requires_grad=True)\nfor i in range(len(y)):\n    y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n\nx, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\nif cuda:\n    # They need to be on the same device and be synchronized.\n    x, y, y_hot = x.cuda(device=0), y.cuda(device=0), y_hot.cuda(device=0)\n    u = u.cuda(device=0)\n\noutputs = net(u)    \noutputs = net(x,y_hot)\n\nx_hat = outputs[\"x_hat\"]\nmu, log_var = outputs[\"mu\"], outputs[\"log_var\"]\nmu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\nz = outputs[\"z\"]\nlogits = outputs[\"y_hat\"]\ny_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\nclassification_loss = torch.sum(torch.abs(y_hot - logits))\n\n#loss, kl = loss_function(x, mu_img, log_var_img, torch.sum(mu,dim = 1), torch.sum(log_var,dim = 1))\nloss, kl = loss_function(x,outputs)\nprint('mu:      ',mu.shape,torch.sum(torch.isnan(mu)))\nprint('log_var: ',log_var.shape,torch.sum(torch.isnan(log_var)))\nprint('mu_img:      ',mu_img.shape,torch.sum(torch.isnan(mu_img)))\nprint('log_var_img: ',log_var_img.shape,torch.sum(torch.isnan(log_var_img)))\nprint('x:           ',x.shape,torch.sum(torch.isnan(x)))\nprint('x_hat:       ',x_hat.shape,torch.sum(torch.isnan(x_hat)))\nprint('z:           ',z.shape,torch.sum(torch.isnan(z)))\nprint('loss:        ',loss)\nprint('kl:          ',kl)\nprint('Class. loss: ',classification_loss)",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "mu:       torch.Size([32, 10, 64]) tensor(0, device='cuda:0')\nlog_var:  torch.Size([32, 10, 64]) tensor(0, device='cuda:0')\nmu_img:       torch.Size([32, 1, 224, 224]) tensor(0, device='cuda:0')\nlog_var_img:  torch.Size([32, 1, 224, 224]) tensor(0, device='cuda:0')\nx:            torch.Size([32, 1, 224, 224]) tensor(0, device='cuda:0')\nx_hat:        torch.Size([32, 1, 224, 224]) tensor(0, device='cuda:0')\nz:            torch.Size([32, 10, 64]) tensor(0, device='cuda:0')\nloss:         tensor(52.8615, device='cuda:0', grad_fn=<NegBackward>)\nkl:           tensor(18.7804, device='cuda:0', grad_fn=<MulBackward0>)\nClass. loss:  tensor(319.9999, device='cuda:0', grad_fn=<SumBackward0>)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "84f3af7ceb3dc3f3c84e6b8f64bf36920ce6c31b"
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n# Deleting variable Image and reload package Image\n%reset_selective -f \"^Image$\"\nfrom IPython.display import Image\nfrom sklearn.manifold import TSNE\nimport sys\n\n\nimport os\nfrom sklearn.decomposition import PCA\n\nnum_epochs = 50 # No_train_samples // batch_size\nbatch_per_epoch = No_train_samples // batch_size\n\ntmp_img = \"tmp_vae_out.png\"\nshow_sampling_points = False\nclasses = [0,1]\n\ntrain_loss, valid_loss, train_acc = [], [], []\ntrain_kl, valid_kl, valid_acc = [], [], []\ntotal_loss = []\nalpha = 0.5\ndevice = torch.device(\"cuda:0\" if cuda else \"cpu\")\nprint(\"Using device:\", device)\n\n\nfor epoch in range(num_epochs):\n    batch_loss, batch_kl, batch_acc = [], [], []\n    net.train()\n    \n    # Go through each batch in the training dataset using the loader\n    # Note that y is not necessarily known as it is here\n    count = 0\n    total_loss_batch = 0\n    for (x, y), (u, _) in zip(cycle(train_loader_labelled), train_loader):\n        y_hot =  torch.zeros([batch_size,2])\n        for i in range(len(y)):\n            y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n            \n        x, y, u, y_hot = Variable(x), Variable(y), Variable(u), Variable(y_hot)\n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y, y_hot = x.cuda(device=0), y.cuda(device=0), y_hot.cuda(device=0)\n            u = u.cuda(device=0)\n\n        count = count + 1\n        #if not count % (batch_per_epoch/4):\n            #print(\"Epoch:\", epoch, \"Batch:\", count,\"/\",batch_per_epoch)\n\n        #### Unlabelled\n        outputs = net(u)\n        elbo_u, kl_u = loss_function(u, outputs)\n\n        \n        #### Labelled\n        outputs = net(x,y_hot)\n        logits = outputs[\"y_hat\"]\n        y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n        classification_loss = torch.sum(torch.abs(y_hot - logits))\n        acc = torch.sum(y_hot.view(-1,2) * logits.view(-1,2), dim = 1).cpu().detach().numpy()\n        batch_acc.append(sum(acc > 0.5) / batch_size*num_samples)\n        #print(batch_acc[-1])\n        #print(classification_loss)\n        elbo_l, kl_l = loss_function(x, outputs)\n        #loss = torch.sum(torch.tensor([elbo_l, elbo_u,classification_loss],requires_grad = True))\n        loss =  elbo_l + elbo_u + alpha * classification_loss\n        total_loss_batch += loss.item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        batch_loss.append(loss.item())\n        batch_kl.append(kl_u.item()+kl_l.item())\n\n    train_acc.append(np.mean(batch_acc))    \n    train_loss.append(np.mean(batch_loss))\n    train_kl.append(np.mean(batch_kl))\n    total_loss.append(total_loss_batch / (batch_size*batch_per_epoch))\n\n    # Evaluate, do not propagate gradients\n    with torch.no_grad():\n        net.eval()\n        \n        # Just load a single batch from the test loader\n        x, y = next(iter(test_loader))\n        \n        y_hot =  torch.zeros([batch_size,2])\n        for i in range(len(y)):\n            y_hot[i] = torch.tensor([0, 1]) if y[i]==1 else torch.tensor([1, 0])\n        x, y, y_hot = Variable(x), Variable(y), Variable(y_hot)\n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y_hot = x.cuda(device=0), y_hot.cuda(device=0)\n        \n        outputs = net(x)\n        x_hat = outputs['x_hat']\n        logits = outputs[\"y_hat\"]\n        z = outputs[\"z\"]\n        \n        y_hot = y_hot.unsqueeze(dim = 1).repeat(1,logits.shape[1],1)\n        acc = torch.sum(y_hot.view(-1,2) * logits.view(-1,2), dim = 1).cpu().detach().numpy()\n        valid_acc.append(sum(acc > 0.5) / batch_size*num_samples)\n        \n        # elbo, kl = loss_function(x_hat, x, mu, log_var)\n        elbo, kl = loss_function(x, outputs)\n        \n        # We save the latent variable and reconstruction for later use\n        # we will need them on the CPU to plot\n        x = x.to(\"cpu\")\n        x_hat = x_hat.to(\"cpu\")\n        z = z.detach().to(\"cpu\").numpy()\n        \n        valid_loss.append(elbo.item())\n        valid_kl.append(kl.item())\n    \n    if epoch == 0:\n        continue\n    \n    # -- Plotting --\n    f, axarr = plt.subplots(4, 2, figsize=(20, 20))\n\n    # Loss\n    ax = axarr[0, 0]\n    ax.set_title(\"ELBO\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Error')\n\n    ax.plot(np.arange(epoch+1), train_loss, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_loss, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n\n    # Latent space\n    ax = axarr[0, 1]\n\n    ax.set_title('Latent space')\n    ax.set_xlabel('Dimension 1')\n    ax.set_ylabel('Dimension 2')\n    \n    if batch_size > 4:\n        rows = 4\n        columns = batch_size // rows\n    else:\n        rows = 2\n        columns = 2\n        \n        \n    \n    span = np.linspace(-4, 4, rows)\n    grid = np.dstack(np.meshgrid(span, span)).reshape(-1, 2)\n    \n    # If you want to use a dimensionality reduction method you can use\n    # for example PCA by projecting on two principal dimensions\n\n    \"\"\" PCA \"\"\"\n    #z = PCA(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    #z = z.reshape(batch_size,num_samples,2)\n    \n    \"\"\" TSNE \"\"\"\n    z = TSNE(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    z = z.reshape(batch_size,num_samples,2)\n    \n    colors = iter(plt.get_cmap('Set1')(np.linspace(0, 1.0, len(classes))))\n    for c in classes:\n        ax.scatter(*z[c == y.numpy()].reshape(-1, 2).T, c=next(colors), marker='o', label=c)\n        \n    if show_sampling_points:\n        ax.scatter(*grid.T, color=\"k\", marker=\"x\", alpha=0.5, label=\"Sampling points\")\n\n    ax.legend()\n    \n    # KL / reconstruction\n    ax = axarr[1, 0]\n    \n    ax.set_title(\"Kullback-Leibler Divergence\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('KL divergence')\n\n\n    ax.plot(np.arange(epoch+1), train_kl, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_kl, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n    \n    # Latent space samples\n    ax = axarr[1, 1]\n    ax.set_title('Samples from latent space')\n    ax.axis('off')\n\n    with torch.no_grad():\n#         epsilon = torch.from_numpy(grid).float().to(device)\n        epsilon = torch.randn(batch_size, latent_features).to(device)\n        # samples = torch.sigmoid(net.decoder(epsilon)).detach()\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = samples[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            #canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = samples[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Inputs\n    ax = axarr[2, 0]\n    ax.set_title('Inputs')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = x[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[2, 1]\n    ax.set_title('Reconstructions: mean')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x_hat[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = x_hat[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[3, 0]\n    ax.set_title('Reconstructions: sigma')\n    ax.axis('off')\n\n    sigma = (torch.exp(outputs[\"x_log_var\"]/2)).detach().to(\"cpu\").numpy()\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x_hat[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = sigma[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n    \n    \n    # Classification Loss\n    ax = axarr[3, 1]\n    ax.set_title(\"Classification Accuracy\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Accuracy (%)')\n    \n    \n    ax.plot(np.arange(epoch+1), train_acc, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_acc, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n    \n    clear_output(wait=True)\n    plt.savefig(tmp_img)\n    plt.close(f)\n    display(Image(filename=tmp_img))\n\n    os.remove(tmp_img)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59443f6fc779d53bfde279b7e8164f9114c7d486"
      },
      "cell_type": "code",
      "source": "print(y.repeat(num_samples,1).shape)\n\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}