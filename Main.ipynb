{
  "cells": [
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a6d0be88f9a8b923e981dc51bed1a982cd84061e"
      },
      "cell_type": "code",
      "source": "# Questions\n# 1. How do we avoid NaN in the output from Encoder?\n# 2. Is loss function correct? \n#    - How do we avoid normal distributed reconstructionS?\n# 3. Do KL divergence only work under the assumption that covariance is diagonal?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a9d0a574f084c07f3a4234e94e1a3e9e7e36b830"
      },
      "cell_type": "code",
      "source": "! pip install pandas\n! pip install pydicom\n! pip install seaborn\n\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom skimage.transform import resize\nfrom skimage.exposure import equalize_hist\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, clear_output\nimport numpy as np\n%matplotlib nbagg\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(sns.dark_palette(\"purple\"))\n\nimport torch\ncuda = torch.cuda.is_available()\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom functools import reduce\n\nimport torch.nn as nn\nfrom torch.nn.functional import softplus\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom itertools import cycle\nimport operator\nfrom torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU, ELU, ConvTranspose2d, MaxUnpool2d, Softmax, Sigmoid\nfrom torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, dropout, dropout2d\nimport time\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false,
        "_uuid": "521a9a785750517bd117ed9b9ecdb1b4acb42b1a"
      },
      "cell_type": "code",
      "source": "# Show files in data\n#!ls data\n\nKAGGLE = True\n\n# display label format\nif KAGGLE:\n    df = pd.read_csv('../input/stage_2_train_labels.csv')\nelse:\n    df = pd.read_csv('data/stage_1_train_labels.csv')\n\n\n\n# Parameters\nbatch_size = 32\nNo_train_samples = 4096\nNo_test_samples = 2048\nNo = 4\npatientId = df['patientId'][No]\nif KAGGLE:\n    dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId\nelse:\n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId\ndcm_data = pydicom.read_file(dcm_file)\n#print(df.iloc[No])\n\n\nIMG_SIZE = 224\nimg_dimension = [IMG_SIZE,IMG_SIZE] # New size of xray images. \nunq, idx = np.unique(df['patientId'], return_index = True) # Get only unique entrances from the provided data (some patients occur multiple times)\n\n# Reshape images and match to corresponding label in new dataframe\nTarget = []\nImage = []\nDo_img_eq = True\n\nprint_every = 100\n\nelapsed_time = 0\ntic = time.clock()\n\nprint(\"Loading training images: 0 /\", No_train_samples)\nfor i in range(0,No_train_samples):\n    Target.append(df.Target[idx[i]]) # Get label  \n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i,\"/\",No_train_samples,  \"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();   \nprint(\"Train data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n        \nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\nfor i in range(0,No_test_samples):\n    Target.append(df.Target[idx[No_train_samples+i]]) # Get label  \n    patientId = df['patientId'][idx[No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_test_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_test_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i, \"/\",No_test_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n\nprint(\"Test data loaded:\", i+1)\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntest_loader = DataLoader(loader, batch_size=batch_size, shuffle = True)\nprint(' ')\n\nTarget = []\nImage = []\ntic = time.clock()\nelapsed_time = 0\ncount_unlabelled = 0\ncount_labelled = 0\ni = 0\nwhile count_labelled<labels_per_class or count_unlabelled<labels_per_class:\n#for i in range(0,No_train_labelled_samples):\n    Target.append(df.Target[idx[No_test_samples+No_train_samples+i]]) # Get label  \n\n    if Target[i]==1:\n        count_labelled = count_labelled + 1\n    else:\n        count_unlabelled = count_unlabelled + 1\n        \n    patientId = df['patientId'][idx[No_test_samples+No_train_samples+i]] # Get patient id from the idx \n    if KAGGLE:\n        dcm_file = '../input/stage_2_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    else:\n        dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect'))#, anti_aliasing=True)) # resize image\n    if Do_img_eq:\n        Image[-1] = equalize_hist(Image[-1])\n        \"\"\"\n        if i % 2:\n            im = Image[-1]\n            im[0:224,0:112] = 1\n            Image[-1] = im\n        if (i+1) % 2:\n            im = Image[-1]\n            im[0:224,112:224] = 0\n            Image[-1] = im\n        \"\"\"\n    # Logging: counting and time remaining\n    if not i % print_every:\n        toc = time.clock()\n        period_time = toc - tic;\n        if not i == 0:\n            elapsed_time = (elapsed_time + period_time)\n            mean_period_time = elapsed_time// ((i)//print_every)\n            minutes = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)//60)\n            seconds = round(mean_period_time*( (No_train_labelled_samples-i)//print_every)%60)\n            print(\"Data loaded:\", i, \"/\",No_train_labelled_samples,\"    Remaining time: \", minutes,\":\", seconds)\n        tic = time.clock();\n    i = i + 1\n\nprint(\"Labelled test data loaded:\", i+1)\n\n\ndef uniform_stratified_sampler(labels, n=None):\n    \"\"\"\n    Stratified sampler that distributes labels uniformly by\n    sampling at most n data points per class\n    \"\"\"\n    from functools import reduce\n    # Only choose digits in n_labels\n    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n\n    # Ensure uniform distribution of labels\n    np.random.shuffle(indices)\n    indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in classes])\n\n    indices = torch.from_numpy(indices)\n    sampler = SubsetRandomSampler(indices)\n    return sampler\n\n# Convert to Tensor\nTarget = torch.Tensor(Target)\nImage = torch.Tensor(Image)\nImage = Image.unsqueeze(1)\n#print(Image.shape)\n\n# Construct DataLoader\nloader = TensorDataset(Image, Target)\ntrain_loader_labelled = DataLoader(loader, batch_size=batch_size,\n                     sampler=uniform_stratified_sampler(Target, labels_per_class),\n                     pin_memory=cuda)\n\nalpha = No_train_labelled_samples / No_train_samples",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "7b0c00888af3a17ed68855cb9e81eb7347dcebf6"
      },
      "cell_type": "code",
      "source": "# Show an example image \nim = Image[5]\nim = im.squeeze(0)\npylab.imshow(im, cmap=pylab.cm.gist_gray)\npylab.axis('off')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e63c105e2448ad9517b0c4204915b983ac7b45cd"
      },
      "cell_type": "code",
      "source": "# Histogram Equalization\n\"\"\"\nfrom IPython.display import Image\nfrom skimage import exposure\nimport pylab\n\nImage = []\nrow_int = []\n\nfor i in range(0,16):\n    patientId = df['patientId'][idx[i]] # Get patient id from the idx \n    dcm_file = 'data/stage_1_train_images/%s.dcm' % patientId # find the image-file corresponding to the patient id\n    dcm_data = pydicom.read_file(dcm_file) # Load the image \n    Image.append(resize(dcm_data.pixel_array, output_shape=img_dimension, mode='reflect', anti_aliasing=True)) # resize image\n    tmp = equalize_hist(Image[-1])\n    row_int.append(np.mean(tmp[:,40:180],axis=1))\n\n# Plotting of images\nf, axarr = plt.subplots(1, 1, figsize=(50, 25))\n\ncolumns = 10\ncanvas = np.zeros((columns*IMG_SIZE, IMG_SIZE*3))\nfor j in range(columns):\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 0:IMG_SIZE] = Image[j]\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , IMG_SIZE:2*IMG_SIZE] = equalize_hist(Image[j])\n    canvas[j*IMG_SIZE:(j+1)*IMG_SIZE , 2*IMG_SIZE:3*IMG_SIZE] = np.matrix.transpose(np.tile(row_int[j],(IMG_SIZE,1)))\n\nplt.imshow(canvas, cmap='gray')\nax = axarr\nax.set_title('Original Images')\nax.axis('off')\nax.grid(False)\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "a24225561ee4d17136a2bea5416d7efa9f2bb3f1"
      },
      "cell_type": "code",
      "source": "# Define size variables\nprint_shapes = False\nIMG_SIZE = 224\nheight = IMG_SIZE\nwidth = IMG_SIZE\nchannels = 1\nnum_features = 224**2\n\n# Regulization\nL2_reg = 1e-6\ndo_p = 0.2\n\n# Conv Layers\nconv_out_channels = [8, 16, 32]\nconv_kernel = [5, 5, 3]\nconv_padding = [2, 2, 1]\nconv_stride = [1, 1, 1]\n\n# MaxPool Layers\npool_kernel = 3\npool_padding = 0\npool_stride = 3\n\n# Fully connected layers\nlin_layer = [1000, 200]\n\n# auxillary parameters\naux_layer = [1000, 200]\naux_variables = 32\naux_in = 2 # layer no. where a is included in encoder\naux_decoder_layers = [100, 100]\n\n# classifier parameters\nclassifier_layer = [1000, 500, 50]\nNo_classes = len(classes)\n\n# No. of layes\nNUM_CONV = len(conv_out_channels)\nNUM_LIN = len(lin_layer)\nNUM_AUX = len(aux_layer)\nNUM_CLASS = len(classifier_layer)\nNUM_AUX_DECODER = len(aux_decoder_layers)\n\n# Calculating the dimensions \ndef compute_conv_dim(height, width, kernel_size, padding_size, stride_size):\n    height_new = int((height - kernel_size + 2 * padding_size) / stride_size + 1)\n    width_new =  int((width  - kernel_size + 2 * padding_size) / stride_size + 1)\n    return [height_new, width_new]\n\ndef compute_final_dimension(height, width, last_num_channels, num_layers):\n    # First conv layer\n    CNN_height = height\n    CNN_width = width\n    for i in range(num_layers):\n        # conv layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, conv_kernel[i], conv_padding[i], conv_stride[i])\n        # maxpool layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, pool_kernel, pool_padding, pool_stride)\n    final_dim = CNN_height * CNN_width * last_num_channels\n    print(final_dim,CNN_height,CNN_width)\n    return [final_dim, CNN_height, CNN_width]\n\ndef normalize(x):\n    tmp = x-torch.min( torch.min(x,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0]\n    if torch.sum(torch.isnan(tmp))>0:\n        print(\"nan of tmp\",torch.sum(torch.isnan(tmp)))\n    return tmp/(torch.max( torch.max(tmp,dim = 2, keepdim = True)[0] ,dim = 3, keepdim = True)[0] + 1e-8)  \n\ndef gaussian_sample(mu,log_var, num_samples, latent_features):    \n    # Don't propagate gradients through randomness\n    with torch.no_grad():\n        batch_size = mu.size(0)\n        epsilon = torch.randn(batch_size, num_samples, latent_features)\n            \n    if cuda:\n        epsilon = epsilon.cuda()\n        \n    sigma = torch.exp(log_var/2)\n        \n    # We will need to unsqueeze to turn\n    # (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n    if len(mu.shape) == 2:\n        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1)\n    else:\n        z = mu + epsilon * sigma\n    return z\n\n######## Image has to be: (num, channels, height, width)!!!! #########\nclass CNN_VAE(nn.Module):\n    \n    def __init__(self, latent_features, num_samples):\n        super(CNN_VAE, self).__init__()\n        \n        self.latent_features = latent_features\n        self.num_samples = num_samples\n        \n        # Calculate final size of the CNN\n        self.final_dim = compute_final_dimension(height,width,conv_out_channels[-1],NUM_CONV)\n        \n        ## Convolutional layers of the encoder\n        input_channels = channels\n        self.encoder = nn.ModuleList()\n        for i in range(NUM_CONV):\n            self.encoder.append(Conv2d( in_channels=input_channels,\n                                            out_channels=conv_out_channels[i],\n                                            kernel_size=conv_kernel[i],\n                                            stride=conv_stride[i],\n                                            padding=conv_padding[i]))\n            self.encoder.append(BatchNorm2d(conv_out_channels[i]))\n            self.encoder.append(ReLU())\n            self.encoder.append(Dropout2d(p=do_p))\n            self.encoder.append(MaxPool2d(  kernel_size=pool_kernel, \n                                        stride=pool_stride,\n                                        padding=pool_padding,\n                                        return_indices = False))\n            input_channels = conv_out_channels[i]\n        self.add_module(\"encoder\",self.encoder)\n        \n        # Fully connected layers from convolutional layers to latent space\n        self.CNN_to_latent = nn.ModuleList()\n        for i in range(NUM_LIN):\n            if i == 0:\n                in_weights = self.final_dim[0]\n            else:\n                in_weights = lin_layer[i-1]\n            self.CNN_to_latent.append(Linear(in_features=in_weights, out_features=lin_layer[i]))\n            self.CNN_to_latent.append(BatchNorm1d(lin_layer[i]))\n            self.CNN_to_latent.append(ReLU())\n            self.CNN_to_latent.append(Dropout(p=do_p))\n        self.CNN_to_latent.append(Linear(in_features=lin_layer[-1]+aux_variables, out_features=latent_features*2))\n        self.CNN_to_latent.append(BatchNorm1d(latent_features*2))\n        self.CNN_to_latent.append(ReLU())\n        self.CNN_to_latent.append(Dropout(p=do_p))\n        self.add_module(\"CNN_to_latent\",self.CNN_to_latent)\n        \n        # Fully connected layers from convolutional layers to aux. variables\n        if aux_variables > 0:\n            self.CNN_to_aux = nn.ModuleList()\n            for i in range(NUM_AUX):\n                if i == 0:\n                    in_weights = self.final_dim[0]\n                else:\n                    in_weights = aux_layer[i-1]\n                self.CNN_to_aux.append(Linear(in_features=in_weights, out_features=aux_layer[i]))\n                self.CNN_to_aux.append(BatchNorm1d(aux_layer[i]))\n                self.CNN_to_aux.append(ReLU())\n                self.CNN_to_aux.append(Dropout(p=do_p))\n\n            self.CNN_to_aux.append(Linear(in_features=aux_layer[-1], out_features=aux_variables*2))\n            self.CNN_to_aux.append(BatchNorm1d(aux_variables*2))\n            self.CNN_to_aux.append(ReLU())\n            self.CNN_to_aux.append(Dropout(p=do_p))\n            self.add_module(\"CNN_to_aux\", self.CNN_to_aux)\n        \n        # auxillary decoder\n        if aux_variables > 0:\n            self.aux_decoder = nn.ModuleList()\n            for i in range(NUM_AUX_DECODER):\n                if i == 0:\n                    in_weights = self.latent_features\n                else:\n                    in_weights = aux_decoder_layers[i-1]\n                self.aux_decoder.append(Linear(in_features=in_weights, out_features=aux_decoder_layers[i]))\n                self.aux_decoder.append(BatchNorm1d(aux_decoder_layers[i]))\n                self.aux_decoder.append(ReLU())\n                self.aux_decoder.append(Dropout(p=do_p))\n\n            self.aux_decoder.append(Linear(in_features=aux_decoder_layers[-1], out_features=aux_variables*2))\n            self.aux_decoder.append(BatchNorm1d(aux_variables*2))\n            self.aux_decoder.append(ReLU())\n            self.aux_decoder.append(Dropout(p=do_p))\n            self.add_module(\"aux_decoder\", self.aux_decoder)\n            \n        '''\n        # Fully connected layers from convolutional layers to classification\n        self.classifier = nn.ModuleList()\n        for i in range(NUM_CLASS):\n            if i == 0:\n                in_weights = self.final_dim[0]\n            else:\n                in_weights = classifier_layer[i-1]\n            self.classifier.append(Linear(in_features=in_weights, out_features=classifier_layer[i]))\n\n        self.classifier.append(Linear(in_features=classifier_layer[-1], out_features = No_classes))\n        self.add_module(\"classifier\", self.classifier)\n        '''\n        \n        # Initialize fully connected layers from latent space to convolutional layers\n        self.latent_to_CNN = nn.ModuleList()\n        self.latent_to_CNN.append(Linear(in_features=latent_features, out_features=lin_layer[-1]))\n        self.latent_to_CNN.append(BatchNorm1d(lin_layer[-1]))\n        self.latent_to_CNN.append(ReLU())\n        self.latent_to_CNN.append(Dropout(p=do_p))\n        for i in reversed(range(NUM_LIN)):\n            if i == 0:\n                out_weights = self.final_dim[0]\n            else:\n                out_weights = lin_layer[i-1]\n            self.latent_to_CNN.append(Linear(in_features=lin_layer[i], out_features=out_weights))\n            self.latent_to_CNN.append(BatchNorm1d(out_weights))\n            self.latent_to_CNN.append(ReLU())\n            self.latent_to_CNN.append(Dropout(p=do_p))\n        self.add_module(\"latent_to_CNN\",self.latent_to_CNN)\n        \n        # Convolutional layers of the decoder\n        self.decoder = nn.ModuleList()\n        for i in reversed(range(NUM_CONV)):\n            if i == 0:\n                output_channels = channels*2\n            else:\n                output_channels = conv_out_channels[i-1] \n            self.decoder.append(ConvTranspose2d(in_channels=conv_out_channels[i],\n                                                out_channels=output_channels,\n                                                kernel_size=conv_kernel[i],\n                                                stride=conv_stride[i],\n                                                padding=conv_padding[i]))\n            self.decoder.append(BatchNorm2d(output_channels))\n            self.decoder.append(ReLU())\n            self.decoder.append(Dropout2d(p=do_p))                        \n        self.add_module(\"encoder\",self.encoder)\n\n    def forward(self, x):\n        outputs = {}\n        self.indices = []\n        self.layer_size = []\n    \n    \n    # convolutional layers of encoder\n        for i in range(len(self.encoder)):\n            if str(type(self.encoder[i])) == \"<class 'torch.nn.modules.pooling.MaxPool2d'>\":\n                self.layer_size.append(x.shape[-1])\n            x = self.encoder[i](x)  \n            \n        if torch.sum(torch.isnan(x))>0:\n            print('output from conv layers is NaN')\n            print(torch.sum(torch.isnan(x)))\n\n        x = x.view(batch_size, -1)\n        \n        if torch.sum(torch.isnan(x))>0:\n            print('output from conv layers is NaN')\n            print(torch.sum(torch.isnan(x)))\n        \n        # Auxillary linear layers\n        if aux_variables > 0:\n            a = x\n            for i in range(len(self.CNN_to_aux)):\n                a = self.CNN_to_aux[i](a)\n            aux_mu, aux_log_var = torch.chunk(a, 2, dim=-1)\n            # sample auxillary variables\n            a = gaussian_sample(aux_mu,aux_log_var,num_samples,latent_features)            \n        \n        # Linear layers to map to latent space\n        for i in range(len(self.CNN_to_latent)):\n            if i == len(self.CNN_to_latent)-4 and aux_variables > 0:\n                x = torch.cat([x.unsqueeze(1).repeat(1,num_samples,1),a],dim=2)\n                # include y and sum after last layer\n                x = x.view(-1,x.shape[-1])\n            x = self.CNN_to_latent[i](x)\n        x = x.view(batch_size,num_samples,latent_features*2) # make correct dimensions\n\n        if torch.sum(torch.isnan(x))>0:\n            print(\"After fully connected layer:\")\n            print(torch.sum(torch.isnan(x)))\n        \n        # Split encoder outputs into a mean and variance vector\n        mu, log_var = torch.chunk(x, 2, dim=-1)\n        \n        # Make sure that the log variance is positive\n        log_var = softplus(log_var)\n        \n        # Sample\n        z = gaussian_sample(mu,log_var,num_samples,latent_features)        \n        print(\"z size\",z.shape)\n        if torch.sum(torch.isnan(z))>0:\n            print(\"z:\")\n            print(torch.sum(torch.isnan(z)))\n        \n        # aux. decoder\n        a = z.view(-1,latent_features)\n        for i in range(len(self.aux_decoder)):\n            a = self.aux_decoder[i](a)\n        a = a.view(batch_size,num_samples,2,-1)\n        print(a.shape)\n        a_mean, a_log_var = torch.chunk(a, 2, dim=2) # the mean and log_var reconstructions from the decoder\n        \n        x_hat = torch.mean(x_hat, dim=1)\n        x_log_var= torch.mean(x_log_var, dim=1)\n        \n        # linear x layers\n        x = z.view(-1,latent_features)\n        for i in range(len(self.latent_to_CNN)):\n            x = self.latent_to_CNN[i](x)\n        x = x.view(-1, conv_out_channels[NUM_CONV-1], self.final_dim[1], self.final_dim[2])\n        \n        # Run through decoder\n        curr_layer = NUM_CONV-1\n        for i in range(len(self.decoder)):\n            if str(type(self.decoder[i])) == \"<class 'torch.nn.modules.conv.ConvTranspose2d'>\":\n                #print(\"Shape of x before deconv. layer: \",x.shape)\n                upsample = nn.Upsample(size = [self.layer_size[curr_layer],self.layer_size[curr_layer]],\n                                      mode = 'bilinear', \n                                      align_corners = False)\n                x = upsample(x)\n                curr_layer -=1\n            x = self.decoder[i](x)\n        \n        x = x.view(batch_size,-1,channels*2,height,width)\n        self.shapes.append(x.shape)   \n        if torch.sum(torch.isnan(x))>0:\n            print(\"After decoder:\")\n            print(torch.sum(torch.isnan(x)))\n\n        x_mean, x_log_var = torch.chunk(x, 2, dim=2) # the mean and log_var reconstructions from the decoder\n        self.shapes.append(x_mean.shape)\n        self.shapes.append(x_log_var.shape)\n        \n        if torch.sum(torch.isnan(x_mean))>0:\n            print(\"Before Normalization: \", torch.sum(torch.isnan(x_mean)))\n        \n        # The original digits are on the scale [0, 1] \n        x_hat = normalize(x_mean)# to scale for showing an image\n        \n        if torch.sum(torch.isnan(x_hat))>0:\n            print(\"After Normalization: \", torch.sum(torch.isnan(x_hat)))\n        \n        x_log_var = softplus(x_log_var)\n        self.shapes.append(x_hat.shape)\n        self.shapes.append(x_log_var.shape) \n        \n        # Mean over samples\n        x_hat = torch.mean(x_hat, dim=1)\n        x_log_var= torch.mean(x_log_var, dim=1)\n        \n        self.shapes.append(x_mean.shape)\n        self.shapes.append(x_log_var.shape)\n           \n        # Resize x_hat from [batch_size, no_features] to [batch_size, channels, height, width]\n        x_hat = x_hat.view( batch_size, 1, height, width)\n        x_log_var = x_log_var.view( batch_size, 1, height, width)\n        \n        outputs[\"x_hat\"] = x_hat # This is used for visulizations only \n        outputs[\"z\"] = z\n        outputs[\"mu\"] = mu\n        outputs[\"log_var\"] = log_var\n        \n        # image recontructions (notice they are outputted as matrices)\n        outputs[\"x_mean\"] = x_hat #torch.reshape(x_mean,(-1,height,width)) # mean reconstructions (for loss!!!)\n        outputs[\"x_log_var\"] = x_log_var #torch.reshape(x_log_var,(-1,height,width)) # log var reconstructions (for loss!!!)\n        \n        \n        # auxillary outputs\n        outputs[\"a_hat\"] = a_hat\n        outputs[\"aux_mu\"] = aux_mu\n        outputs[\"aux_log_var\"] = aux_log_var\n        outputs[\"a_mean\"] = a_mean\n        outputs[\"a_log_var\"] = a_log_var\n        \n        \n        if print_shapes:\n            print(self.shapes)\n            \n        return outputs\n\n# The number of samples used then initialising the VAE, \n# is number of samples drawn from the distribution\nnum_samples = 10\nlatent_features = 32\n\nnet = CNN_VAE(latent_features, num_samples)\nprint(net)\n# Transfer model to GPU ifavailable\nif cuda:\n    net = net.cuda()\n\n## test\nx = torch.randn(32,1,224,224)\nx = Variable(x)\nif cuda:\n    x = x.cuda()\ny = net(x)\nprint(y['x_hat'].shape)\n#for parameter in net.parameters():\n#    print(parameter.shape)\n#epsilon = torch.randn(batch_size, latent_features).cuda\n#samples = torch.sigmoid(net.decoder(net.latent_to_CNN(epsilon))).detach()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "07a3987893818d88392626a9715ca205cc4630e7"
      },
      "cell_type": "code",
      "source": "from torch.nn.functional import binary_cross_entropy\nfrom torch import optim\nimport math\n\ndef Gaussian_density(sample_img,mu_img,log_var_img):\n    c = - 0.5 * math.log(2 * math.pi)\n    density = c - log_var_img/2 - (sample_img - mu_img)**2/(2 * torch.exp(log_var_img))\n    #print(\"Density:\",density)\n    #print(\"Density.shape:\", density.shape)\n    return torch.sum(density,dim = 1) # Sum over channels\n\ndef ELBO_loss(sample_img, mu_img, log_var_img, mu, log_var):\n    \n    # Reconstruction error, log[p(x|z)]\n    # Sum over features\n        # Old code\n        #likelihood = -binary_cross_entropy(y, t, reduction=\"none\")\n        #likelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n\n    # New code with guassian density\n    likelihood = Gaussian_density(sample_img, mu_img, log_var_img)\n    #likelihood = -binary_cross_entropy(mu_img,sample_img, reduction=\"none\")\n    #ikelihood = likelihood.view(likelihood.size(0), -1).sum(1)\n    \n    # Regularization error: \n    # Kulback-Leibler divergence between approximate posterior, q(z|x)\n    # and prior p(z) = N(z | mu, sigma*I).\n    \n    # In the case of the KL-divergence between diagonal covariance Gaussian and \n    # a standard Gaussian, an analytic solution exists. Using this excerts a lower\n    # variance estimator of KL(q||p)\n    kl = -0.5 * torch.sum(1 + log_var - mu**2 - torch.exp(log_var), dim=1)\n\n    # Combining the two terms in the evidence lower bound objective (ELBO) \n    # mean over batch\n    #print(\"mu.shape\", mu.shape)\n    #print(\"kl.shape:\", kl.shape)\n    #print(\"Likelihood shape:\", likelihood.shape)\n    tmp = likelihood.view(batch_size, -1)\n    tmp = torch.sum(tmp, dim=1) # Sum over features (224x224 = 50.176)\n    #print(\"Likelihood sum shape:\", tmp.shape)\n    #print(\"kl.mean shape:\", kl.shape)\n    \n    ELBO = torch.mean(tmp) - torch.mean(kl)\n\n    # Notice minus sign as we want to maximise ELBO\n    return -ELBO, kl.sum()\n\n\n# Define optimizer: The Adam optimizer works really well with VAEs.\noptimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay= 1e-4)\nloss_function = ELBO_loss",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false,
        "_uuid": "a273c0cf31bfad705dd5c1e1f09ed6a2b1e2ca9c"
      },
      "cell_type": "code",
      "source": "from torch.autograd import Variable\n\nx, _ = next(iter(train_loader))\nx = Variable(x)\nif cuda:\n    x = x.cuda()\n\noutputs = net(x)\n\nx_hat = outputs[\"x_hat\"]\nmu, log_var = outputs[\"mu\"], outputs[\"log_var\"]\nmu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\nz = outputs[\"z\"]\n\nloss, kl = loss_function(x, mu_img, log_var_img, torch.sum(mu,dim = 1), torch.sum(log_var,dim = 1))\n\nprint('mu:      ',mu.shape,torch.sum(torch.isnan(mu)))\nprint('log_var: ',log_var.shape,torch.sum(torch.isnan(log_var)))\nprint('mu_img:      ',mu_img.shape,torch.sum(torch.isnan(mu_img)))\nprint('log_var_img: ',log_var_img.shape,torch.sum(torch.isnan(log_var_img)))\nprint('x:           ',x.shape,torch.sum(torch.isnan(x)))\nprint('x_hat:       ',x_hat.shape,torch.sum(torch.isnan(x_hat)))\nprint('z:           ',z.shape,torch.sum(torch.isnan(z)))\nprint('loss:        ',loss)\nprint('kl:          ',kl)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false,
        "_uuid": "76d69784f7de1b9ba60928892b1d679bed4bbb71"
      },
      "cell_type": "code",
      "source": "# Deleting variable Image and reload package Image\n%reset_selective -f \"^Image$\"\nfrom IPython.display import Image\nfrom sklearn.manifold import TSNE\n\n\nimport os\nfrom sklearn.decomposition import PCA\n\n\nnum_epochs = 50 # No_train_samples // batch_size\nbatch_per_epoch = No_train_samples // batch_size\n\ntmp_img = \"tmp_vae_out.png\"\nshow_sampling_points = False\nclasses = [0,1]\n\ntrain_loss, valid_loss = [], []\ntrain_kl, valid_kl = [], []\ntotal_loss = []\n\ndevice = torch.device(\"cuda:0\" if cuda else \"cpu\")\nprint(\"Using device:\", device)\n\n\nfor epoch in range(num_epochs):\n    batch_loss, batch_kl = [], []\n    net.train()\n    \n    # Go through each batch in the training dataset using the loader\n    # Note that y is not necessarily known as it is here\n    count = 0\n    total_loss_batch = 0\n    for (x, y), (u, _) in zip(cycle(train_loader_labelled), train_loader):\n        x, y, u = Variable(x), Variable(y), Variable(u)\n        \n        if cuda:\n            # They need to be on the same device and be synchronized.\n            x, y = x.cuda(device=0), y.cuda(device=0)\n            u = u.cuda(device=0)\n            \n        # x = Variable(x)\n        count = count + 1\n        #if not count % (batch_per_epoch/4):\n            #print(\"Epoch:\", epoch, \"Batch:\", count,\"/\",batch_per_epoch)\n        \n        # This is an alternative way of putting\n        # a tensor on the GPU\n        #x = x.to(device)\n           \n        #### Unlabelled   \n        outputs = net(u)\n        x_hat = outputs['x_hat']\n        mu, log_var = outputs['mu'], outputs['log_var']\n        mu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\n        elbo_u, kl = loss_function(u, mu_img, log_var_img, mu, log_var)\n        \n        #### Labelled\n        outputs = net(x)\n        x_classified = outputs['x_classified']\n\n        if No_train_labelled_samples>batch_size:\n            no_labels = batch_size\n            logits = torch.zeros([1, batch_size],requires_grad=True)\n        else:\n            no_labels = No_train_labelled_samples\n            logits = torch.zeros([1, No_train_labelled_samples], requires_grad=True)\n            \n        if cuda:\n            logits = logits.cuda(device=0)\n            \n        for i in range(0,no_labels):\n            loggitters, value = max(enumerate(x_classified[i]), key=operator.itemgetter(1))\n            logits[0,i] = loggitters\n            if x_classified[0,1] == 0.5:\n                logits[i] = 2\n         \n        #print(\"x_classified:\", x_classified.shape)\n        #print(\"y:\",y.shape)\n        #print(\"Classification:\", classification_loss)\n        #classification_loss = torch.sum(y * torch.log(logits + 1e-8)).mean()\n        classification_loss = torch.sum(torch.abs(y - logits))\n        \n        elbo = 5000*alpha * classification_loss + elbo_u \n        \n        total_loss_batch += classification_loss\n        \n        optimizer.zero_grad()\n        elbo.backward()\n        optimizer.step()\n        \n        batch_loss.append(elbo.item())\n        batch_kl.append(kl.item())\n\n    train_loss.append(np.mean(batch_loss))\n    train_kl.append(np.mean(batch_kl))\n    total_loss.append(total_loss_batch / (batch_size*batch_per_epoch))\n\n    # Evaluate, do not propagate gradients\n    with torch.no_grad():\n        net.eval()\n        \n        # Just load a single batch from the test loader\n        x, y = next(iter(test_loader))\n        x = Variable(x)\n        \n        x = x.to(device)\n        \n        outputs = net(x)\n        x_hat = outputs['x_hat']\n        mu, log_var = outputs['mu'], outputs['log_var']\n        mu_img, log_var_img = outputs[\"x_mean\"], outputs[\"x_log_var\"]\n        z = outputs[\"z\"]\n    \n        # elbo, kl = loss_function(x_hat, x, mu, log_var)\n        elbo, kl = loss_function(x, mu_img, log_var_img, mu, log_var)\n        \n        # We save the latent variable and reconstruction for later use\n        # we will need them on the CPU to plot\n        x = x.to(\"cpu\")\n        x_hat = x_hat.to(\"cpu\")\n        z = z.detach().to(\"cpu\").numpy()\n        \n        valid_loss.append(elbo.item())\n        valid_kl.append(kl.item())\n    \n    if epoch == 0:\n        continue\n    \n    # -- Plotting --\n    f, axarr = plt.subplots(4, 2, figsize=(20, 20))\n\n    # Loss\n    ax = axarr[0, 0]\n    ax.set_title(\"ELBO\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Error')\n\n    ax.plot(np.arange(epoch+1), train_loss, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_loss, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n\n    # Latent space\n    ax = axarr[0, 1]\n\n    ax.set_title('Latent space')\n    ax.set_xlabel('Dimension 1')\n    ax.set_ylabel('Dimension 2')\n    \n    if batch_size > 4:\n        rows = 4\n        columns = batch_size // rows\n    else:\n        rows = 2\n        columns = 2\n        \n        \n    \n    span = np.linspace(-4, 4, rows)\n    grid = np.dstack(np.meshgrid(span, span)).reshape(-1, 2)\n    \n    # If you want to use a dimensionality reduction method you can use\n    # for example PCA by projecting on two principal dimensions\n\n    \"\"\" PCA \"\"\"\n    #z = PCA(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    #z = z.reshape(batch_size,num_samples,2)\n    \n    \"\"\" TSNE \"\"\"\n    z = TSNE(n_components=2).fit_transform(z.reshape(-1,latent_features))\n    z = z.reshape(batch_size,num_samples,2)\n    \n    \n#     print(z.shape)\n#     print(z.reshape(-1,latent_features).shape)\n    colors = iter(plt.get_cmap('Set1')(np.linspace(0, 1.0, len(classes))))\n    for c in classes:\n        ax.scatter(*z[c == y.numpy()].reshape(-1, 2).T, c=next(colors), marker='o', label=c)\n        \n    if show_sampling_points:\n        ax.scatter(*grid.T, color=\"k\", marker=\"x\", alpha=0.5, label=\"Sampling points\")\n\n    ax.legend()\n    \n    # KL / reconstruction\n    ax = axarr[1, 0]\n    \n    ax.set_title(\"Kullback-Leibler Divergence\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('KL divergence')\n\n\n    ax.plot(np.arange(epoch+1), train_kl, color=\"black\")\n    ax.plot(np.arange(epoch+1), valid_kl, color=\"gray\", linestyle=\"--\")\n    ax.legend(['Training', 'Validation'])\n    \n    # Latent space samples\n    ax = axarr[1, 1]\n    ax.set_title('Samples from latent space')\n    ax.axis('off')\n\n    with torch.no_grad():\n#         epsilon = torch.from_numpy(grid).float().to(device)\n        epsilon = torch.randn(batch_size, latent_features).to(device)\n        samples = torch.sigmoid(net.decoder(epsilon)).detach()\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = samples[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = samples[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Inputs\n    ax = axarr[2, 0]\n    ax.set_title('Inputs')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = x[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[2, 1]\n    ax.set_title('Reconstructions: mean')\n    ax.axis('off')\n\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x_hat[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = x_hat[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n\n    # Reconstructions\n    ax = axarr[3, 0]\n    ax.set_title('Reconstructions: sigma')\n    ax.axis('off')\n\n    sigma = torch.exp(log_var_img/2)\n    canvas = np.zeros((IMG_SIZE*rows, columns*IMG_SIZE))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i % columns + rows * j\n            #temp_img = x_hat[idx].reshape((224, 224))\n            #canvas[i*28:(i+1)*28, j*28:(j+1)*28] = resize(temp_img, output_shape=[28,28], mode='reflect', anti_aliasing=True)\n            canvas[i*IMG_SIZE:(i+1)*IMG_SIZE, j*IMG_SIZE:(j+1)*IMG_SIZE] = sigma[idx,0:IMG_SIZE**2].reshape((IMG_SIZE, IMG_SIZE))\n    ax.imshow(canvas, cmap='gray')\n    \n    \n    # Classification Loss\n    ax = axarr[3, 1]\n    ax.set_title(\"Classification Error\")\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Error')\n\n    ax.plot(np.arange(epoch+1), total_loss, color=\"black\")\n    ax.legend(['Training'])\n    \n    \n    plt.savefig(tmp_img)\n    plt.close(f)\n    display(Image(filename=tmp_img))\n    #clear_output(wait=True)\n\n    os.remove(tmp_img)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}