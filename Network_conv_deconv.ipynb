{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "816d60c9d9e444e178292ad9477b8b2cfc6672b8"
      },
      "cell_type": "code",
      "source": "! pip install pandas\n! pip install pydicom\n! pip install seaborn\n\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom skimage.transform import resize\nfrom skimage.exposure import equalize_hist\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, clear_output\nimport numpy as np\n%matplotlib nbagg\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_palette(sns.dark_palette(\"purple\"))\n\nimport torch\ncuda = torch.cuda.is_available()\n\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nfrom functools import reduce\n\nimport torch.nn as nn\nfrom torch.nn.functional import softplus\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d, ReLU, ELU,ConvTranspose2d, MaxUnpool2d\nfrom torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax, dropout, dropout2d\nimport time",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (0.23.4)\nRequirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.15.4)\nRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.4)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\nRequirement already satisfied: pydicom in /opt/conda/lib/python3.6/site-packages (1.2.0)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.6/site-packages (0.9.0)\nRequirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.1.0)\nRequirement already satisfied: numpy>=1.9.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.15.4)\nRequirement already satisfied: pandas>=0.15.2 in /opt/conda/lib/python3.6/site-packages (from seaborn) (0.23.4)\nRequirement already satisfied: matplotlib>=1.4.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2018.4)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2.6.0)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.1.0)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4ae8ed33b9bd26017e7972b81a5e8041bccf17f"
      },
      "cell_type": "code",
      "source": "# Define size variables\nIMG_SIZE = 224\nNUM_CONV = 4\nheight = IMG_SIZE\nwidth = IMG_SIZE\nchannels = 1\nnum_features = 224**2\n\n# Regulization\nL2_reg = 1e-6\ndo_p = 0.2\n# Conv Layers\nconv_out_channels = [8, 16, 32, 64]\nconv_kernel = [14, 11, 5, 5]\nconv_padding = [7, 5, 2, 2]\nconv_stride = [2, 2, 1, 1]\n\n# MaxPool Layers\npool_kernel = 4\npool_padding = 2\npool_stride = 2\n\n# Calculating the dimensions \ndef compute_conv_dim(height, width, kernel_size, padding_size, stride_size):\n    height_new = int((height - kernel_size + 2 * padding_size) / stride_size + 1)\n    width_new =  int((width  - kernel_size + 2 * padding_size) / stride_size + 1)\n    return [height_new, width_new]\n\ndef compute_final_dimension(height, width, last_num_channels, num_layers):\n    # First conv layer\n    CNN_height = height\n    CNN_width = width\n    for i in range(num_layers):\n        # conv layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, conv_kernel[i], conv_padding[i], conv_stride[i])\n        # maxpool layer\n        CNN_height, CNN_width = compute_conv_dim(CNN_height, CNN_width, pool_kernel, pool_padding, pool_stride)\n    final_dim = CNN_height * CNN_width * last_num_channels\n    print(CNN_height,CNN_width)\n    return [final_dim, CNN_height, CNN_width]\n    \n######## Image has to be: (num, channels, height, width)!!!! #########\nclass CNN_VAE(nn.Module):\n    \n    def __init__(self, latent_features, num_samples):\n        super(CNN_VAE, self).__init__()\n        \n        self.latent_features = latent_features\n        self.num_samples = num_samples\n        \n        # Calculate final size of the CNN\n        self.final_dim = compute_final_dimension(height,width,conv_out_channels[-1],4)\n        ## CNN encoder\n        self.conv_layers = []\n        self.batchnorm = []\n        self.indices = []\n        input_channels = channels\n        for i in range(NUM_CONV):\n            self.conv_layers.append(Conv2d( in_channels=input_channels,\n                                            out_channels=conv_out_channels[i],\n                                            kernel_size=conv_kernel[i],\n                                            stride=conv_stride[i],\n                                            padding=conv_padding[i]))\n            self.batchnorm.append(BatchNorm2d(conv_out_channels[i]))\n            input_channels = conv_out_channels[i]\n        self.relu = ReLU();\n        self.dropout = Dropout2d(p=do_p)\n        self.maxPool_layers = MaxPool2d(  kernel_size=pool_kernel, \n                                        stride=pool_stride,\n                                        padding=pool_padding,\n                                        return_indices = True)\n        \n        self.CNN_to_latent = Linear(in_features=self.final_dim[0], out_features=self.latent_features*2)\n        self.latent_to_CNN = Linear(in_features=self.latent_features, out_features=self.final_dim[0])\n        # The latent code must be decoded into the original image\n        self.deconv_layers = []\n        self.batchnorm_deconv = []\n        for i in reversed(range(NUM_CONV)):\n            if i == 0:\n                output_channels = channels*2\n            else:\n                output_channels = conv_out_channels[i-1]\n                \n            self.deconv_layers.append(ConvTranspose2d(  in_channels=conv_out_channels[i],\n                                                        out_channels=output_channels,\n                                                        kernel_size=conv_kernel[i],\n                                                        stride=conv_stride[i],\n                                                        padding=conv_padding[i]))\n            self.batchnorm_deconv.append(BatchNorm2d(output_channels))\n        \n        self.maxunpool = MaxUnpool2d(   kernel_size=pool_kernel, \n                                        stride=pool_stride,\n                                        padding=pool_padding)       \n        \n\n    def forward(self, x): \n        outputs = {}\n        self.indices = []\n        self.layer_size = []\n        for i in range(NUM_CONV):\n            print(\"Shape of x after \",i,\"'th layer: \",x.shape)\n            x = self.conv_layers[i]((x))\n            self.layer_size.append(x.shape[-1])\n            x, tmp_indices = self.maxPool_layers(x) \n            self.indices.append(tmp_indices)\n            x = self.relu(x)\n            x = self.batchnorm[i]((x))\n            x = self.dropout(x)\n        print(\"Shape of x after \",i+1,\"'th layer: \",x.shape)\n        \n        #x = self.encoder(x)\n        if torch.sum(torch.isnan(x))>0:\n            print('output from encoder is NaN')\n            print(torch.sum(torch.isnan(x)))\n\n        batch_size = x.size(0)\n        x = x.view( batch_size, -1)\n        # x = x.view(num_samples,-1) # fold out the CNN layers\n        x = self.CNN_to_latent(x)\n        if torch.sum(torch.isnan(x))>0:\n            print(\"After fully connected layer:\")\n            print(torch.sum(torch.isnan(x)))\n     \n        # Split encoder outputs into a mean and variance vector\n        mu, log_var = torch.chunk(x, 2, dim=-1)\n        \n        # Make sure that the log variance is positive\n        log_var = softplus(log_var)\n        \n        # :- Reparametrisation trick\n        # a sample from N(mu, sigma) is mu + sigma * epsilon\n        # where epsilon ~ N(0, 1)\n                \n        # Don't propagate gradients through randomness\n        with torch.no_grad():\n            batch_size = mu.size(0)\n            epsilon = torch.randn(batch_size, self.num_samples, self.latent_features)\n            \n            if cuda:\n                epsilon = epsilon.cuda()\n        \n        sigma = torch.exp(log_var/2)\n        \n        # We will need to unsqueeze to turn (batch_size, latent_dim) -> (batch_size, 1, latent_dim)\n        z = mu.unsqueeze(1) + epsilon * sigma.unsqueeze(1) \n        if torch.sum(torch.isnan(z))>0:\n            print(\"z:\")\n            print(torch.sum(torch.isnan(z)))\n                  \n        print(\"z.shape before view\", z.shape)\n        x = z.view(-1,latent_features)\n        print(\"x.shape after view\", x.shape)\n        x = self.latent_to_CNN(x)\n        print(\"x.shape after latent2CNN\", x.shape)\n        x = x.view(batch_size*num_samples, conv_out_channels[NUM_CONV-1], self.final_dim[1], self.final_dim[2])\n        print(\"x.shape input to decoder:\", x.shape)\n         # Run through decoder\n        print(self.indices[1].shape)\n        for i in range(NUM_CONV):\n            print(\"Shape of x after \", i,\"'th layer: \",x.shape)\n            x = self.maxunpool(x,self.indices[NUM_CONV-1-i].repeat(num_samples,1,1,1),output_size=[self.layer_size[NUM_CONV-1-i],self.layer_size[NUM_CONV-1-i]]) \n            x = self.deconv_layers[i]((x))\n            x = self.relu(x)\n            x = self.batchnorm_deconv[i]((x))\n            x = self.dropout(x)\n        print(\"Shape of x after \", i+1,\"'th layer: \",x.shape)    \n        x = x.view(batch_size,num_samples*2,height,width)\n        \n        print(\"Shape of x after deconvolution: \",x.shape)    \n        if torch.sum(torch.isnan(x))>0:\n            print(\"After decoder:\")\n            print(torch.sum(torch.isnan(x)))\n\n        x_mean, x_log_var = torch.chunk(x, 2, dim=1) # the mean and log_var reconstructions from the decoder\n        \n        # The original digits are on the scale [0, 1]\n        x_hat = torch.sigmoid(x_mean) # to scale for showing an image\n        x_log_var = softplus(x_log_var)\n        \n        # Mean over samples\n        x_hat = torch.mean(x_hat, dim=1)\n        x_log_var= torch.mean(x_log_var, dim=1)\n        \n        # Resize x_hat from [batch_size, no_features] to [batch_size, channels, height, width]\n        x_hat = x_hat.view( batch_size, 1, height, width)\n        x_log_var = x_log_var.view( batch_size, 1, height, width)\n        \n        outputs[\"x_hat\"] = x_hat # This is used for visulizations only \n        outputs[\"z\"] = z\n        outputs[\"mu\"] = mu\n        outputs[\"log_var\"] = log_var\n        \n        # image recontructions (notice they are outputted as matrices)\n        outputs[\"x_mean\"] = x_hat #torch.reshape(x_mean,(-1,height,width)) # mean reconstructions (for loss!!!)\n        outputs[\"x_log_var\"] = x_log_var #torch.reshape(x_log_var,(-1,height,width)) # log var reconstructions (for loss!!!)\n        \n        return outputs\n\n\nlatent_features = 4\n# The number of samples used then initialising the VAE, \n# is number of samples drawn from the distribution\nnum_samples = 10\n\nnet = CNN_VAE(latent_features, num_samples)\n#print(net)\n# Transfer model to GPU if available\nif cuda:\n    net = net.cuda()\nx = torch.randn(32, 1, 224,224)\ny = net.forward(x)",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": "5 5\nShape of x after  0 'th layer:  torch.Size([32, 1, 224, 224])\nShape of x after  1 'th layer:  torch.Size([32, 8, 57, 57])\nShape of x after  2 'th layer:  torch.Size([32, 16, 15, 15])\nShape of x after  3 'th layer:  torch.Size([32, 32, 8, 8])\nShape of x after  4 'th layer:  torch.Size([32, 64, 5, 5])\nz.shape before view torch.Size([32, 10, 4])\nx.shape after view torch.Size([320, 4])\nx.shape after latent2CNN torch.Size([320, 1600])\nx.shape input to decoder: torch.Size([320, 64, 5, 5])\ntorch.Size([32, 16, 15, 15])\nShape of x after  0 'th layer:  torch.Size([320, 64, 5, 5])\nShape of x after  1 'th layer:  torch.Size([320, 32, 8, 8])\nShape of x after  2 'th layer:  torch.Size([320, 16, 15, 15])\nShape of x after  3 'th layer:  torch.Size([320, 8, 57, 57])\nShape of x after  4 'th layer:  torch.Size([320, 2, 224, 224])\nShape of x after deconvolution:  torch.Size([32, 20, 224, 224])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cec7e5a31894a397d3098d710091d6a2eb2a8c7"
      },
      "cell_type": "code",
      "source": "x = torch.randn(1, 2)\nprint(x)\nx = x.repeat(10,1)\nprint(x)\n",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": "tensor([[0.0510, 0.0154]])\ntensor([[0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154],\n        [0.0510, 0.0154]])\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}